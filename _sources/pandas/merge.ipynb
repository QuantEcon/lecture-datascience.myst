{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de65a536",
   "metadata": {},
   "source": [
    "# Merge\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- {doc}`Reshape <reshape>`\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Know the different pandas routines for combining datasets\n",
    "- Know when to use `pd.concat` vs `pd.merge` vs `pd.join`\n",
    "- Be able to apply the three main combining routines \n",
    "\n",
    "**Data**\n",
    "\n",
    "- WDI data on GDP components, population, and square miles of countries\n",
    "- Book ratings: 6,000,000 ratings for the 10,000 most rated books on\n",
    "  [Goodreads](https://www.goodreads.com/)\n",
    "- Details for all delayed US domestic flights in November 2016,\n",
    "  obtained from the [Bureau of Transportation\n",
    "  Statistics](https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp)\n",
    "\n",
    "\n",
    "```{literalinclude} ../_static/colab_light.raw\n",
    "```\n",
    "\n",
    "## Combining Datasets\n",
    "\n",
    "Often, we will want perform joint analysis on data from different sources.\n",
    "\n",
    "For example, when analyzing the regional sales for a company, we might\n",
    "want to include industry aggregates or demographic information for each\n",
    "region.\n",
    "\n",
    "Or perhaps we are working with product-level data, have a list of\n",
    "product groups in a separate dataset, and want to compute aggregate\n",
    "statistics for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcf29e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from WDI. Units trillions of 2010 USD\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://datascience.quantecon.org/assets/data/wdi_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m wdi \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m wdi\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m      6\u001b[0m wdi2017 \u001b[38;5;241m=\u001b[39m wdi\u001b[38;5;241m.\u001b[39mxs(\u001b[38;5;241m2017\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# from WDI. Units trillions of 2010 USD\n",
    "url = \"https://datascience.quantecon.org/assets/data/wdi_data.csv\"\n",
    "wdi = pd.read_csv(url).set_index([\"country\", \"year\"])\n",
    "wdi.info()\n",
    "\n",
    "wdi2017 = wdi.xs(2017, level=\"year\")\n",
    "wdi2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8756ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi2016_17 = wdi.loc[pd.IndexSlice[:, [2016, 2017]],: ]\n",
    "wdi2016_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://www.nationmaster.com/country-info/stats/Geography/Land-area/Square-miles\n",
    "# units -- millions of square miles\n",
    "sq_miles = pd.Series({\n",
    "   \"United States\": 3.8,\n",
    "   \"Canada\": 3.8,\n",
    "   \"Germany\": 0.137,\n",
    "   \"United Kingdom\": 0.0936,\n",
    "   \"Russia\": 6.6,\n",
    "}, name=\"sq_miles\").to_frame()\n",
    "sq_miles.index.name = \"country\"\n",
    "sq_miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from WDI. Units millions of people\n",
    "pop_url = \"https://datascience.quantecon.org/assets/data/wdi_population.csv\"\n",
    "pop = pd.read_csv(pop_url).set_index([\"country\", \"year\"])\n",
    "pop.info()\n",
    "pop.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536692a9",
   "metadata": {},
   "source": [
    "Suppose that we were asked to compute a number of statistics with the data above:\n",
    "\n",
    "- As a measure of land usage or productivity, what is Consumption per square mile?\n",
    "- What is GDP per capita (per person) for each country in each year? How about\n",
    "  Consumption per person?\n",
    "- What is the population density of each country? How much does it change over time?\n",
    "\n",
    "Notice that to answer any of the questions from above, we will have to use data\n",
    "from more than one of our DataFrames.\n",
    "\n",
    "In this lecture, we will learn many techniques for combining datasets that\n",
    "originate from different sources, careful to ensure that data is properly\n",
    "aligned.\n",
    "\n",
    "In pandas three main methods can combine datasets:\n",
    "\n",
    "1. `pd.concat([dfs...])`\n",
    "1. `pd.merge(df1, df2)`\n",
    "1. `df1.join(df2)`\n",
    "\n",
    "We'll look at each one.\n",
    "\n",
    "## `pd.concat`\n",
    "\n",
    "The `pd.concat` function is used to stack two or more DataFrames\n",
    "together.\n",
    "\n",
    "An example of when you might want to do this is if you have monthly data\n",
    "in separate files on your computer and would like to have 1 year of data\n",
    "in a single DataFrame.\n",
    "\n",
    "The first argument to `pd.concat` is a list of DataFrames to be\n",
    "stitched together.\n",
    "\n",
    "The other commonly used argument is named `axis`.\n",
    "\n",
    "As we have seen before, many pandas functions have an `axis` argument\n",
    "that specifies whether a particular operation should happen down rows\n",
    "(`axis=0`) or along columns (`axis=1`).\n",
    "\n",
    "In the context of `pd.concat`, setting `axis=0` (the default case)\n",
    "will stack DataFrames on top of one another while `axis=1` stacks them\n",
    "side by side.\n",
    "\n",
    "We'll look at each case separately.\n",
    "\n",
    "### `axis=0`\n",
    "\n",
    "When we call `pd.concat` and set `axis=0`, the list of DataFrames\n",
    "passed in the first argument will be stacked on top of one another.\n",
    "\n",
    "Let's try it out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c61c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to pd.concat([wdi2017, sq_miles]) -- axis=0 is default\n",
    "pd.concat([wdi2017, sq_miles], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed0800",
   "metadata": {},
   "source": [
    "Notice a few things:\n",
    "\n",
    "The number of rows in the output is the total number\n",
    ": of rows in all inputs. The labels are all from the original\n",
    "  DataFrames.\n",
    "\n",
    "- The column labels are all the distinct column labels from all the inputs.\n",
    "- For columns that appeared only in one input, the value for all row labels\n",
    "  originating from a different input is equal to `NaN` (marked as missing).\n",
    "\n",
    "### `axis=1`\n",
    "\n",
    "In this example, concatenating by stacking\n",
    "side-by-side makes more sense.\n",
    "\n",
    "We accomplish this by passing `axis=1` to `pd.concat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333850eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([wdi2017, sq_miles], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c5b13",
   "metadata": {},
   "source": [
    "Notice here that\n",
    "\n",
    "- The index entries are all unique index entries that appeared in any DataFrame.\n",
    "- The column labels are all column labels from the inputs.\n",
    "- As `wdi2017` didn't have a `Russia` row, the value for all of its columns\n",
    "  is `NaN`.\n",
    "\n",
    "Now we can answer one of our questions from above: What is\n",
    "Consumption per square mile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([wdi2017, sq_miles], axis=1)\n",
    "temp[\"Consumption\"] / temp[\"sq_miles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69856c5f",
   "metadata": {},
   "source": [
    "## `pd.merge`\n",
    "\n",
    "`pd.merge` operates on two DataFrames at a time and is primarily used\n",
    "to bring columns from one DataFrame into another, *aligning* data based\n",
    "on one or more \"key\" columns.\n",
    "\n",
    "This is a somewhat difficult concept to grasp by reading, so let's look at some\n",
    "examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7beaa",
   "metadata": {},
   "source": [
    "The output here looks very similar to what we saw with `concat` and\n",
    "`axis=1`, except that the row for `Russia` does not appear.\n",
    "\n",
    "We will talk more about why this happened soon.\n",
    "\n",
    "For now, let's look at a slightly more intriguing example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5100ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2016_17, sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef08e0",
   "metadata": {},
   "source": [
    "Here's how we think about what happened:\n",
    "\n",
    "- The data in `wdi2016_17` is copied over exactly as is.\n",
    "- Because `country` was on the index for both DataFrames, it is on the\n",
    "  index of the output.\n",
    "- We lost the year on the index -- we'll work on getting it back below.\n",
    "- The additional column in `sq_miles` was added to column labels for the\n",
    "  output.\n",
    "- The data from the `sq_miles` column was added to the output by looking up\n",
    "  rows where the `country` in the two DataFrames lined up.\n",
    "  -  Note that all the countries appeared twice, and the data in `sq_miles` was repeated. This is because `wdi2016_17` had two rows for each country.\n",
    "  -  Also note that because `Russia` did not appear in `wdi2016_17`, the value `sq_miles.loc[\"Russia\"]` (i.e. `6.6`) is not used the output.\n",
    "\n",
    "How do we get the year back?\n",
    "\n",
    "We must first call `reset_index` on `wdi2016_17` so\n",
    "that in the first step when all columns are copied over, `year` is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2016_17.reset_index(), sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f7b5b",
   "metadata": {},
   "source": [
    "### Multiple Columns\n",
    "\n",
    "Sometimes, we need to merge multiple columns.\n",
    "\n",
    "For example our `pop` and `wdi2016_17` DataFrames both have observations\n",
    "organized by country and year.\n",
    "\n",
    "To properly merge these datasets, we would need to align the data by\n",
    "both country and year.\n",
    "\n",
    "We pass a list to the `on` argument to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0db7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2016_17, pop, on=[\"country\", \"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50062726",
   "metadata": {},
   "source": [
    "Now, we can answer more of our questions from above: What is GDP per capita (per\n",
    "person) for each country in each year? How about Consumption per person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_pop = pd.merge(wdi2016_17, pop, on=[\"country\", \"year\"])\n",
    "wdi_pop[\"GDP\"] / wdi_pop[\"Population\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80392c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_pop[\"Consumption\"] / wdi_pop[\"Population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5dfde",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir1\n",
    "See exercise 1 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "\n",
    "## Arguments to `merge`\n",
    "\n",
    "The `pd.merge` function can take many optional arguments.\n",
    "\n",
    "We'll talk about a few of the most commonly-used ones here and refer you\n",
    "to the\n",
    "[documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html#pandas.merge)\n",
    "for more details.\n",
    "\n",
    "We'll follow the pandas convention and refer to the first argument to\n",
    "`pd.merge` as `left` and call the second `right`.\n",
    "\n",
    "### `on`\n",
    "\n",
    "We have already seen this one used before, but we want to point out that on\n",
    "is optional.\n",
    "\n",
    "If nothing is given for this argument, pandas will use **all** columns\n",
    "in `left` and `right` with the same name.\n",
    "\n",
    "In our example, `country` is the only column that appears in both\n",
    "DataFrames, so it is used for `on` if we don't pass anything.\n",
    "\n",
    "The following two are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41766099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we move index back to columns, the `on` is un-necessary\n",
    "pd.merge(wdi2017.reset_index(), sq_miles.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b54b7",
   "metadata": {},
   "source": [
    "### `left_on`, `right_on`\n",
    "\n",
    "Above, we used the `on` argument to identify a column in both `left`\n",
    "and `right` that was used to align data.\n",
    "\n",
    "Sometimes, both DataFrames don't have the same name for this column.\n",
    "\n",
    "In that case, we use the `left_on` and `right_on` arguments, passing\n",
    "the proper column name(s) to align the data.\n",
    "\n",
    "We'll show you an example below, but it is somewhat silly as our\n",
    "DataFrames do both have the `country` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, sq_miles, left_on=\"country\", right_on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247b6f4",
   "metadata": {},
   "source": [
    "### `left_index`, `right_index`\n",
    "\n",
    "Sometimes, as in our example, the key used to align data is actually in the\n",
    "index instead of one of the columns.\n",
    "\n",
    "In this case, we can use the `left_index` or `right_index` arguments.\n",
    "\n",
    "We should only set these values to a boolean (`True` or `False`).\n",
    "\n",
    "Let's practice with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa074cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(wdi2017, sq_miles, left_on=\"country\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2c931",
   "metadata": {},
   "source": [
    "### `how`\n",
    "\n",
    "The `how` is perhaps the most powerful, but most conceptually\n",
    "difficult of the arguments we will cover.\n",
    "\n",
    "This argument controls which values from the key column(s) appear in the\n",
    "output.\n",
    "\n",
    "The 4 possible options for this argument are summarized in\n",
    "the image below.\n",
    "\n",
    "```{figure} ../_static/merge_venns.png\n",
    ":alt: merge\\_venns.png\n",
    "```\n",
    "\n",
    "In words, we have:\n",
    "\n",
    "- `left`: Default and what we described above. It uses\n",
    "  the keys from the `left` DataFrame.\n",
    "- `right`: Output will contain all keys from `right`.\n",
    "- `inner`: The output will only contain keys that appear in *both*\n",
    "  `left` and `right`.\n",
    "- `outer`: The output will contain any key found in either `left`\n",
    "  or `right`.\n",
    "\n",
    "In addition to the above, we will use the following two DataFrames to\n",
    "illustrate the `how` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da76f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi2017_no_US = wdi2017.drop(\"United States\")\n",
    "wdi2017_no_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_miles_no_germany = sq_miles.drop(\"Germany\")\n",
    "sq_miles_no_germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e647517",
   "metadata": {},
   "source": [
    "Now, let's see all the possible `how` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a619d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default\n",
    "pd.merge(wdi2017, sq_miles, on=\"country\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f76397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice ``Russia`` is included\n",
    "pd.merge(wdi2017, sq_miles, on=\"country\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice no United States or Russia\n",
    "pd.merge(wdi2017_no_US, sq_miles, on=\"country\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes all 5, even though they don't all appear in either DataFrame\n",
    "pd.merge(wdi2017_no_US, sq_miles_no_germany, on=\"country\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726036df",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir2\n",
    "See exercise 2 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir3\n",
    "See exercise 3 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "\n",
    "### `df.merge(df2)`\n",
    "\n",
    "Note that the DataFrame type has a `merge` *method*.\n",
    "\n",
    "It is the same as the function we have been working with, but passes the\n",
    "DataFrame before the period as `left`.\n",
    "\n",
    "Thus `df.merge(other)` is equivalent to `pd.merge(df, other)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi2017.merge(sq_miles, on=\"country\", how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db3cb7",
   "metadata": {},
   "source": [
    "## `df.join`\n",
    "\n",
    "The `join` method for a DataFrame is very similar to the `merge`\n",
    "method described above, but only allows you to use the index of the\n",
    "`right` DataFrame as the join key.\n",
    "\n",
    "Thus, `left.join(right, on=\"country\")` is equivalent to calling\n",
    "`pd.merge(left, right, left_on=\"country\", right_index=True)`.\n",
    "\n",
    "The implementation of the `join` method calls `merge` internally,\n",
    "but sets the `left_on` and `right_index` arguments for you.\n",
    "\n",
    "You can do anything with `df.join` that you can do with\n",
    "`df.merge`, but `df.join` is more convenient to use if the keys of `right`\n",
    "are in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi2017.join(sq_miles, on=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi2017.merge(sq_miles, left_on=\"country\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a12562",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "\n",
    "Let's put these tools to practice by loading some real datasets and\n",
    "seeing how these functions can be applied.\n",
    "\n",
    "We'll analyze ratings of books from the website [Goodreads](https://www.goodreads.com/).\n",
    "\n",
    "We accessed the data [here](https://github.com/zygmuntz/goodbooks-10k).\n",
    "\n",
    "Let's load it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf093605",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/goodreads_ratings.csv.zip\"\n",
    "ratings = pd.read_csv(url)\n",
    "display(ratings.head())\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8418557",
   "metadata": {},
   "source": [
    "We can already do some interesting things with just the ratings data.\n",
    "\n",
    "Let's see how many ratings of each number are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"rating\"].value_counts().sort_index().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22add03c",
   "metadata": {},
   "source": [
    "Let's also see how many users have rated `N` books, for all `N`\n",
    "possible.\n",
    "\n",
    "To do this, we will use `value_counts` twice (can you think of why?).\n",
    "\n",
    "We will see a more flexible way of performing similar grouped operations in\n",
    "a future lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_by_n = (\n",
    "    ratings[\"user_id\"]\n",
    "    .value_counts()  # Series. Index: user_id, value: n ratings by user\n",
    "    .value_counts()  # Series. Index: n_ratings by user, value: N_users with this many ratings\n",
    "    .sort_index()    # Sort our Series by the index (number of ratings)\n",
    "    .reset_index()   # Dataframe with columns `index` (from above) and `user_id`\n",
    "    .rename(columns={\"index\": \"N_ratings\", \"user_id\": \"N_users\"})\n",
    ")\n",
    "users_by_n.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106fbd1",
   "metadata": {},
   "source": [
    "Let's look at some statistics on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3fd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_by_n.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc87bdb",
   "metadata": {},
   "source": [
    "We can see the same data visually in a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_by_n.plot(kind=\"box\", subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceefced",
   "metadata": {},
   "source": [
    "Let's practice applying the want operator...\n",
    "\n",
    "**Want**: Determine whether a relationship between the number of\n",
    "ratings a user has written and the distribution of the ratings exists. (Maybe we\n",
    "are an author hoping to inflate our ratings and wonder if we should\n",
    "target \"more experienced\" Goodreads users, or focus on newcomers.)\n",
    "\n",
    "Let's start from the result and work our way backwards:\n",
    "\n",
    "1. We can answer our question if we have two similar DataFrames:\n",
    "    - All ratings by the `N` (e.g. 5) users with the most ratings\n",
    "    - All ratings by the `N` users with the least number of\n",
    "      ratings\n",
    "1. To get that, we will need to extract rows of `ratings` with\n",
    "   `user_id` associated with the `N` most and least prolific raters\n",
    "1. For that, we need the most and least active `user_id`s\n",
    "1. To get that info, we need a count of how many ratings each user left.\n",
    "    - We can get that with `df[\"user_id\"].value_counts()`, so let's\n",
    "      start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27799e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4\n",
    "n_ratings = ratings[\"user_id\"].value_counts()\n",
    "n_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30871d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "N = 5\n",
    "most_prolific_users = n_ratings.nlargest(5).index.tolist()\n",
    "least_prolific_users = n_ratings.nsmallest(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "active_ratings = ratings.loc[ratings[\"user_id\"].isin(most_prolific_users), :]\n",
    "inactive_ratings = ratings.loc[ratings[\"user_id\"].isin(least_prolific_users), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 -- get the answer!\n",
    "active_ratings[\"rating\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", title=\"Distribution of ratings by most active users\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_ratings[\"rating\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", title=\"Distribution of ratings by least active users\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5e2a5",
   "metadata": {},
   "source": [
    "Nice! From the picture above, the new users look much more\n",
    "likely to leave 5 star ratings than more experienced users.\n",
    "\n",
    "### Book Data\n",
    "\n",
    "We know what you are probably thinking: \"Isn't this a lecture on merging?\n",
    "Why are we only using one dataset?\"\n",
    "\n",
    "We hear you.\n",
    "\n",
    "Let's also load a dataset containing information on the actual books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/goodreads_books.csv\"\n",
    "books = pd.read_csv(url)\n",
    "# we only need a few of the columns\n",
    "books = books[[\"book_id\", \"authors\", \"title\"]]\n",
    "print(\"shape: \", books.shape)\n",
    "print(\"dtypes:\\n\", books.dtypes, sep=\"\")\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5985c1b",
   "metadata": {},
   "source": [
    "We could do similar interesting things with just the books dataset,\n",
    "but we will skip it for now and merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_books = pd.merge(ratings, books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f60e98",
   "metadata": {},
   "source": [
    "Now, let's see which books have been most often rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_books_id = rated_books[\"book_id\"].value_counts().nlargest(10).index\n",
    "most_rated_books = rated_books.loc[rated_books[\"book_id\"].isin(most_rated_books_id), :]\n",
    "list(most_rated_books[\"title\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520909d",
   "metadata": {},
   "source": [
    "Let's use our `pivot_table` knowledge to compute the average rating\n",
    "for each of these books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da754591",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_books.pivot_table(values=\"rating\", index=\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2550ea",
   "metadata": {},
   "source": [
    "These ratings seem surprisingly low, given that they are the most often\n",
    "rated books on Goodreads.\n",
    "\n",
    "I wonder what the bottom of the distribution looks like...\n",
    "\n",
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir4\n",
    "See exercise 4 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "\n",
    "Let's compute the average number of ratings for each book in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ratings = (\n",
    "    rated_books\n",
    "    .pivot_table(values=\"rating\", index=\"title\")\n",
    "    .sort_values(by=\"rating\", ascending=False)\n",
    ")\n",
    "average_ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda2354",
   "metadata": {},
   "source": [
    "What does the overall distribution of average ratings look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0448b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a kernel density estimate of average ratings\n",
    "average_ratings.plot.density(xlim=(1, 5))\n",
    "\n",
    "# or a histogram\n",
    "average_ratings.plot.hist(bins=30, xlim=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f830203",
   "metadata": {},
   "source": [
    "It looks like most books have an average rating of just below 4.\n",
    "\n",
    "## Extra Example: Airline Delays\n",
    "\n",
    "Let's look at one more example.\n",
    "\n",
    "This time, we will use a dataset from the [Bureau of Transportation\n",
    "Statistics](https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp)\n",
    "that describes the cause of all US domestic flight delays\n",
    "in November 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fdaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/airline_performance_dec16.csv.zip\"\n",
    "air_perf = pd.read_csv(url)[[\"CRSDepTime\", \"Carrier\", \"CarrierDelay\", \"ArrDelay\"]]\n",
    "air_perf.info()\n",
    "air_perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ebb61",
   "metadata": {},
   "source": [
    "The `Carrier` column identifies the airline and the `CarrierDelay`\n",
    "reports the number of minutes of the total delay assigned as the\n",
    "\"carrier's fault\".\n",
    "\n",
    "**Want**: Determine which airlines, on average, contribute most to\n",
    "delays.\n",
    "\n",
    "We can do this using `pivot_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_delays = (\n",
    "    air_perf\n",
    "    .pivot_table(index=\"Carrier\", values=\"CarrierDelay\", aggfunc=\"mean\")\n",
    "    .sort_values(\"CarrierDelay\")\n",
    "    .nlargest(10, \"CarrierDelay\")\n",
    ")\n",
    "avg_delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce7c06",
   "metadata": {},
   "source": [
    "The one issue with this dataset is that we don't know what all those two\n",
    "letter carrier codes are!\n",
    "\n",
    "Thankfully, we have a second dataset that maps the two letter code\n",
    "into the full airline name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1304e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/airline_carrier_codes.csv.zip\"\n",
    "carrier_code = pd.read_csv(url)\n",
    "carrier_code.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7bc84",
   "metadata": {},
   "source": [
    "Let's merge these names so we know which airlines we should avoid\n",
    "flying..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_delays_w_code = pd.merge(avg_delays,carrier_code, left_on=\"Carrier\", right_on=\"Code\")\n",
    "avg_delays_w_code.sort_values(\"CarrierDelay\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836392e",
   "metadata": {},
   "source": [
    "Based on that information, which airlines would you avoid near\n",
    "the holidays?\n",
    "\n",
    "## Visualizing Merge Operations\n",
    "\n",
    "As we did in the {doc}`reshape lecture <reshape>`, we will visualize the\n",
    "various merge operations using artificial DataFrames.\n",
    "\n",
    "First, we create some dummy DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6511ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL = pd.DataFrame(\n",
    "    {\"Key\": [\"A\", \"B\", \"A\", \"C\"], \"C1\":[1, 2, 3, 4], \"C2\": [10, 20, 30, 40]},\n",
    "    index=[\"L1\", \"L2\", \"L3\", \"L4\"]\n",
    ")[[\"Key\", \"C1\", \"C2\"]]\n",
    "\n",
    "print(\"This is dfL: \")\n",
    "display(dfL)\n",
    "\n",
    "dfR = pd.DataFrame(\n",
    "    {\"Key\": [\"A\", \"B\", \"C\", \"D\"], \"C3\": [100, 200, 300, 400]},\n",
    "    index=[\"R1\", \"R2\", \"R3\", \"R4\"]\n",
    ")[[\"Key\", \"C3\"]]\n",
    "\n",
    "print(\"This is dfR:\")\n",
    "display(dfR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fce49",
   "metadata": {},
   "source": [
    "### `pd.concat`\n",
    "\n",
    "Recall that calling `pd.concat(..., axis=0)` will stack DataFrames on top of\n",
    "one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfL, dfR], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d54a4a",
   "metadata": {},
   "source": [
    "Here's how we might visualize that.\n",
    "\n",
    "```{figure} ../_static/concat_axis0.gif\n",
    ":alt: concat\\_axis0.gif\n",
    "```\n",
    "\n",
    "We can also set `axis=1` to stack side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dfL, dfR], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8739fcd",
   "metadata": {},
   "source": [
    "Here's how we might visualize that.\n",
    "\n",
    "```{figure} ../_static/concat_axis1.gif\n",
    ":alt: concat\\_axis1.gif\n",
    "```\n",
    "\n",
    "### `pd.merge`\n",
    "\n",
    "The animation below shows a visualization of what happens when we call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3262b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(dfL, dfR, on=\"Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c908dc",
   "metadata": {},
   "source": [
    "```{figure} ../_static/left_merge.gif\n",
    ":alt: left\\_merge.gif\n",
    "```\n",
    "\n",
    "Now, let's focus on what happens when we set `how=\"right\"`.\n",
    "\n",
    "Pay special attention to what happens when filling the output value for\n",
    "the key `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(dfL, dfR, on=\"Key\", how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602a166",
   "metadata": {},
   "source": [
    "```{figure} ../_static/right_merge.gif\n",
    ":alt: right\\_merge.gif\n",
    "```\n",
    "\n",
    "### Exercises With Artificial Data\n",
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir5\n",
    "See exercise 5 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir6\n",
    "See exercise 6 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "````{admonition} Exercise\n",
    ":name: pd-mrg-dir7\n",
    "See exercise 7 in the {ref}`exercise list <pd-mrg-ex>`.\n",
    "````\n",
    "\n",
    "(pd-mrg-ex)=\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Use your new `merge` skills to answer the final question from above: What\n",
    "is the population density of each country? How much does it change over\n",
    "time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13395e60",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir1>`)\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Compare the `how=\"left\"` with `how=\"inner\"` options using the\n",
    "DataFrames `wdi2017_no_US` and `sq_miles_no_germany`.\n",
    "\n",
    "Are the different? How?\n",
    "\n",
    "Will this happen for all pairs of DataFrames, or are `wdi2017_no_US` and\n",
    "`sq_miles_no_germany` special in some way?\n",
    "\n",
    "Also compare `how=\"right\"` and `how=\"outer\"` and answer the same\n",
    "questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7659cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e64e0",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir2>`)\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Can you pick the correct argument for `how` such that `pd.merge(wdi2017, sq_miles,\n",
    "how=\"left\")` is equal to `pd.merge(sq_miles, wdi2017, how=XXX)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706813fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0865b",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir3>`)\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Repeat the analysis above to determine the average rating for the books with the\n",
    "*least* number ratings.\n",
    "\n",
    "Is there a distinguishable difference in the average rating compared to\n",
    "the most rated books?\n",
    "\n",
    "Did you recognize any of the books?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07700a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c4381",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir4>`)\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "In writing, describe what the output looks like when you do\n",
    "`pd.concat([dfL, dfR], axis=1)` (see above and/or run the cell below).\n",
    "\n",
    "Be sure to describe things like:\n",
    "\n",
    "- What are the columns? What about columns with the same name?\n",
    "- What is the index?\n",
    "- Do any `NaN`s get introduced? If so, where? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818270f",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "pd.concat([dfL, dfR], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387461ba",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir5>`)\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "Determine what happens when you run each of the two cells below.\n",
    "\n",
    "For each cell, answer the list of questions from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c10573",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# First code cell for above exercise\n",
    "pd.concat([dfL, dfL], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fb084",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Second code cell for above exercise\n",
    "pd.concat([dfR, dfR], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02495210",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir6>`)\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Describe in words why the output of `pd.merge(dfL, dfR,\n",
    "how=\"right\")` has more rows than either `dfL` or `dfR`.\n",
    "\n",
    "Run the cell below to see the output of that operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dee76",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "pd.merge(dfL, dfR, how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbbd1f",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-mrg-dir7>`)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   10,
   49,
   56,
   66,
   71,
   85,
   91,
   146,
   149,
   168,
   170,
   182,
   185,
   196,
   198,
   207,
   209,
   229,
   231,
   245,
   247,
   252,
   257,
   259,
   291,
   295,
   298,
   313,
   315,
   328,
   330,
   360,
   365,
   368,
   372,
   377,
   382,
   387,
   390,
   410,
   412,
   430,
   434,
   436,
   449,
   454,
   460,
   462,
   472,
   482,
   486,
   488,
   492,
   494,
   516,
   522,
   529,
   535,
   542,
   546,
   560,
   568,
   573,
   575,
   579,
   583,
   588,
   590,
   604,
   611,
   615,
   621,
   634,
   639,
   650,
   658,
   666,
   670,
   675,
   678,
   690,
   706,
   713,
   715,
   725,
   727,
   739,
   741,
   752,
   754,
   783,
   785,
   802,
   804,
   813,
   815,
   829,
   831,
   846,
   849,
   859,
   865,
   869,
   880,
   883
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}