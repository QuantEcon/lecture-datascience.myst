{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b70582c",
   "metadata": {},
   "source": [
    "# Basic Functionality\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- {doc}`pandas Intro <intro>`\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Be familiar with `datetime`\n",
    "- Use built-in aggregation functions and be able to create your own and\n",
    "  apply them using `agg`\n",
    "- Use built-in Series transformation functions and be able to create your\n",
    "  own and apply them using `apply`\n",
    "- Use built-in scalar transformation functions and be able to create your\n",
    "  own and apply them using `applymap`\n",
    "- Be able to select subsets of the DataFrame using boolean selection\n",
    "- Know what the \"want operator\" is and how to apply it\n",
    "\n",
    "**Data**\n",
    "\n",
    "- US state unemployment data from Bureau of Labor Statistics\n",
    "\n",
    "\n",
    "```{literalinclude} ../_static/colab_light.raw\n",
    "```\n",
    "\n",
    "## State Unemployment Data\n",
    "\n",
    "In this lecture, we will use unemployment data by state at a monthly\n",
    "frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e3071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7052b",
   "metadata": {},
   "source": [
    "First, we will download the data directly from a url and read it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ad2062",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Load up the data -- this will take a couple seconds\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://datascience.quantecon.org/assets/data/state_unemployment.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m unemp_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "## Load up the data -- this will take a couple seconds\n",
    "url = \"https://datascience.quantecon.org/assets/data/state_unemployment.csv\"\n",
    "unemp_raw = pd.read_csv(url, parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c319777",
   "metadata": {},
   "source": [
    "The pandas `read_csv` will determine most datatypes of the underlying columns.  The\n",
    "exception here is that we need to give pandas a hint so it can load up the `Date` column as a Python datetime type: the `parse_dates=[\"Date\"]`.\n",
    "\n",
    "We can see the basic structure of the downloaded data by getting the first 5 rows, which directly matches\n",
    "the underlying CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1f0ce",
   "metadata": {},
   "source": [
    "Note that a row has a date, state, labor force size, and unemployment rate.\n",
    "\n",
    "For our analysis, we want to look at the unemployment rate across different states over time, which\n",
    "requires a transformation of the data similar to an Excel pivot-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c80a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about the details here quite yet\n",
    "unemp_all = (\n",
    "    unemp_raw\n",
    "    .reset_index()\n",
    "    .pivot_table(index=\"Date\", columns=\"state\", values=\"UnemploymentRate\")\n",
    ")\n",
    "unemp_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347238dc",
   "metadata": {},
   "source": [
    "Finally, we can filter it to look at a subset of the columns (i.e. \"state\" in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\n",
    "    \"Arizona\", \"California\", \"Florida\", \"Illinois\",\n",
    "    \"Michigan\", \"New York\", \"Texas\"\n",
    "]\n",
    "unemp = unemp_all[states]\n",
    "unemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257757e",
   "metadata": {},
   "source": [
    "When plotting, a DataFrame knows the column and index names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.plot(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020da993",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: pd-bas-dir1\n",
    "See exercise 1 in the {ref}`exercise list <pd-bas-ex>`.\n",
    "````\n",
    "\n",
    "## Dates in pandas\n",
    "\n",
    "You might have noticed that our index now has a nice format for the\n",
    "dates (`YYYY-MM-DD`) rather than just a year.\n",
    "\n",
    "This is because the `dtype` of the index is a variant of `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fae63",
   "metadata": {},
   "source": [
    "We can index into a DataFrame with a `DatetimeIndex` using string\n",
    "representations of dates.\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data corresponding to a single date\n",
    "unemp.loc[\"01/01/2000\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be998798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for all days between New Years Day and June first in the year 2000\n",
    "unemp.loc[\"01/01/2000\":\"06/01/2000\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661764d",
   "metadata": {},
   "source": [
    "We will learn more about what pandas can do with dates and times in an\n",
    "upcoming lecture on time series data.\n",
    "\n",
    "## DataFrame Aggregations\n",
    "\n",
    "Let's talk about *aggregations*.\n",
    "\n",
    "Loosely speaking, an aggregation is an operation that combines multiple\n",
    "values into a single value.\n",
    "\n",
    "For example, computing the mean of three numbers (for example\n",
    "`[0, 1, 2]`) returns a single number (1).\n",
    "\n",
    "We will use aggregations extensively as we analyze and manipulate our data.\n",
    "\n",
    "Thankfully, pandas makes this easy!\n",
    "\n",
    "### Built-in Aggregations\n",
    "\n",
    "pandas already has some of the most frequently used aggregations.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Mean  (`mean`)\n",
    "- Variance (`var`)\n",
    "- Standard deviation (`std`)\n",
    "- Minimum (`min`)\n",
    "- Median (`median`)\n",
    "- Maximum (`max`)\n",
    "- etc...\n",
    "\n",
    "```{note}\n",
    "When looking for common operations, using \"tab completion\" goes a long way.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25246a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37479702",
   "metadata": {},
   "source": [
    "As seen above, the aggregation's default is to aggregate each column.\n",
    "\n",
    "However, by using the `axis` keyword argument, you can do aggregations by\n",
    "row as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed87597",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.var(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797b53f",
   "metadata": {},
   "source": [
    "### Writing Your Own Aggregation\n",
    "\n",
    "The built-in aggregations will get us pretty far in our analysis, but\n",
    "sometimes we need more flexibility.\n",
    "\n",
    "We can have pandas perform custom aggregations by following these two\n",
    "steps:\n",
    "\n",
    "1. Write a Python function that takes a `Series` as an input and\n",
    "   outputs a single value.\n",
    "1. Call the `agg` method with our new function as an argument.\n",
    "\n",
    "For example, below, we will classify states as \"low unemployment\" or\n",
    "\"high unemployment\" based on whether their mean unemployment level is\n",
    "above or below 6.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 1: We write the (aggregation) function that we'd like to use\n",
    "#\n",
    "def high_or_low(s):\n",
    "    \"\"\"\n",
    "    This function takes a pandas Series object and returns high\n",
    "    if the mean is above 6.5 and low if the mean is below 6.5\n",
    "    \"\"\"\n",
    "    if s.mean() < 6.5:\n",
    "        out = \"Low\"\n",
    "    else:\n",
    "        out = \"High\"\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 2: Apply it via the agg method.\n",
    "#\n",
    "unemp.agg(high_or_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this differ from unemp.agg(high_or_low)?\n",
    "unemp.agg(high_or_low, axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff131",
   "metadata": {},
   "source": [
    "Notice that `agg` can also accept multiple functions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.agg([min, max, high_or_low])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7211e81",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: pd-bas-dir2\n",
    "See exercise 2 in the {ref}`exercise list <pd-bas-ex>`.\n",
    "````\n",
    "\n",
    "## Transforms\n",
    "\n",
    "Many analytical operations do not necessarily involve an aggregation.\n",
    "\n",
    "The output of a function applied to a Series might need to be a new\n",
    "Series.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- Compute the percentage change in unemployment from month to month.\n",
    "- Calculate the cumulative sum of elements in each column.\n",
    "\n",
    "### Built-in Transforms\n",
    "\n",
    "pandas comes with many transform functions including:\n",
    "\n",
    "- Cumulative sum/max/min/product (`cum(sum|min|max|prod)`)\n",
    "- Difference  (`diff`)\n",
    "- Elementwise addition/subtraction/multiplication/division (`+`, `-`, `*`, `/`)\n",
    "- Percent change (`pct_change`)\n",
    "- Number of occurrences of each distinct value (`value_counts`)\n",
    "- Absolute value (`abs`)\n",
    "\n",
    "Again, tab completion is helpful when trying to find these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ea11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.pct_change(fill_method = None).head() # Skip calculation for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp.diff().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989732b",
   "metadata": {},
   "source": [
    "Transforms can be split into to several main categories:\n",
    "\n",
    "1. *Series transforms*: functions that take in one Series and produce another Series. The index of the input and output does not need to be the same.\n",
    "1. *Scalar transforms*: functions that take a single value and produce a single value. An example is the `abs` method, or adding a constant to each value of a Series.\n",
    "\n",
    "### Custom Series Transforms\n",
    "\n",
    "pandas also simplifies applying custom Series transforms to a Series or the\n",
    "columns of a DataFrame. The steps are:\n",
    "\n",
    "1. Write a Python function that takes a Series and outputs a new Series.\n",
    "1. Pass our new function as an argument to the `apply` method (alternatively, the `transform` method).\n",
    "\n",
    "As an example, we will standardize our unemployment data to have mean 0\n",
    "and standard deviation 1.\n",
    "\n",
    "After doing this, we can use an aggregation to determine at which date the\n",
    "unemployment rate is most different from \"normal times\" for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53df71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 1: We write the Series transform function that we'd like to use\n",
    "#\n",
    "def standardize_data(x):\n",
    "    \"\"\"\n",
    "    Changes the data in a Series to become mean 0 with standard deviation 1\n",
    "    \"\"\"\n",
    "    mu = x.mean()\n",
    "    std = x.std()\n",
    "\n",
    "    return (x - mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Step 2: Apply our function via the apply method.\n",
    "#\n",
    "std_unemp = unemp.apply(standardize_data)\n",
    "std_unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the absolute value of all elements of a function\n",
    "abs_std_unemp = std_unemp.abs()\n",
    "\n",
    "abs_std_unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the date when unemployment was \"most different from normal\" for each State\n",
    "def idxmax(x):\n",
    "    # idxmax of Series will return index of maximal value\n",
    "    return x.idxmax()\n",
    "\n",
    "abs_std_unemp.agg(idxmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa5723",
   "metadata": {},
   "source": [
    "### Custom Scalar Transforms\n",
    "\n",
    "As you may have predicted, we can also apply custom scalar transforms to our\n",
    "pandas data.\n",
    "\n",
    "To do this, we use the following pattern:\n",
    "\n",
    "1. Define a Python function that takes in a scalar and produces a scalar.\n",
    "1. Pass this function as an argument to the `applymap` Series or DataFrame method.\n",
    "\n",
    "Complete the exercise below to practice writing and using your own scalar\n",
    "transforms.\n",
    "\n",
    "````{admonition} Exercise\n",
    ":name: pd-bas-dir3\n",
    "See exercise 3 in the {ref}`exercise list <pd-bas-ex>`.\n",
    "````\n",
    "\n",
    "## Boolean Selection\n",
    "\n",
    "We have seen how we can select subsets of data by referring to the index\n",
    "or column names.\n",
    "\n",
    "However, we often want to select based on conditions met by\n",
    "the data itself.\n",
    "\n",
    "Some examples are:\n",
    "\n",
    "- Restrict analysis to all individuals older than 18.\n",
    "- Look at data that corresponds to particular time periods.\n",
    "- Analyze only data that corresponds to a recession.\n",
    "- Obtain data for a specific product or customer ID.\n",
    "\n",
    "We will be able to do this by using a Series or list of boolean values\n",
    "to index into a Series or DataFrame.\n",
    "\n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small = unemp.head()  # Create smaller data so we can see what's happening\n",
    "unemp_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e23824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of booleans selects rows\n",
    "unemp_small.loc[[True, True, True, False, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second argument selects columns, the  ``:``  means \"all\".\n",
    "# here we use it to select all columns\n",
    "unemp_small.loc[[True, False, True, False, True], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d91ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use booleans to select both rows and columns\n",
    "unemp_small.loc[[True, True, True, False, False], [True, False, False, False, False, True, True]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8e8dd",
   "metadata": {},
   "source": [
    "### Creating Boolean DataFrames/Series\n",
    "\n",
    "We can use {doc}`conditional statements <../python_fundamentals/control_flow>` to\n",
    "construct Series of booleans from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small[\"Texas\"] < 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1eedd",
   "metadata": {},
   "source": [
    "Once we have our Series of bools, we can use it to extract subsets of\n",
    "rows from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small.loc[unemp_small[\"Texas\"] < 4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small[\"New York\"] > unemp_small[\"Texas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_NY = unemp_small[\"New York\"] > unemp_small[\"Texas\"]\n",
    "unemp_small.loc[big_NY]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137681f",
   "metadata": {},
   "source": [
    "#### Multiple Conditions\n",
    "\n",
    "In the boolean section of the {doc}`basics lecture <../python_fundamentals/basics>`, we saw\n",
    "that we can use the words `and` and `or` to combine multiple booleans into\n",
    "a single bool.\n",
    "\n",
    "Recall:\n",
    "\n",
    "- `True and False -> False`\n",
    "- `True and True -> True`\n",
    "- `False and False -> False`\n",
    "- `True or False -> True`\n",
    "- `True or True -> True`\n",
    "- `False or False -> False`\n",
    "\n",
    "We can do something similar in pandas, but instead of\n",
    "`bool1 and bool2` we write:\n",
    "\n",
    "```{code-block} python\n",
    "(bool_series1) & (bool_series2)\n",
    "```\n",
    "\n",
    "Likewise, instead of `bool1 or bool2` we write:\n",
    "\n",
    "```{code-block} python\n",
    "(bool_series1) | (bool_series2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_NYTX = (unemp_small[\"Texas\"] < 4.7) & (unemp_small[\"New York\"] < 4.7)\n",
    "small_NYTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small[small_NYTX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484118d",
   "metadata": {},
   "source": [
    "#### `isin`\n",
    "\n",
    "Sometimes, we will want to check whether a data point takes on one of a\n",
    "several fixed values.\n",
    "\n",
    "We could do this by writing `(df[\"x\"] == val_1) | (df[\"x\"] == val_2)`\n",
    "(like we did above), but there is a better way: the `.isin` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_small[\"Michigan\"].isin([3.3, 3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now select full rows where this Series is True\n",
    "unemp_small.loc[unemp_small[\"Michigan\"].isin([3.3, 3.2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf41ce",
   "metadata": {},
   "source": [
    "#### `.any` and `.all`\n",
    "\n",
    "Recall from the boolean section of the {doc}`basics lecture <../python_fundamentals/basics>`\n",
    "that the Python functions `any` and `all` are aggregation functions that\n",
    "take a collection of booleans and return a single boolean.\n",
    "\n",
    "`any` returns True whenever at least one of the inputs are True while\n",
    "`all` is True only when all the inputs are `True`.\n",
    "\n",
    "Series and DataFrames with `dtype` bool have `.any` and `.all`\n",
    "methods that apply this logic to pandas objects.\n",
    "\n",
    "Let's use these methods to count how many months all the states in our\n",
    "sample had high unemployment.\n",
    "\n",
    "As we work through this example, consider the [\"want\n",
    "operator\"](http://albertjmenkveld.com/2014/07/07/endogeneous-price-dispersion/), a helpful\n",
    "concept from Nobel Laureate [Tom\n",
    "Sargent](http://www.tomsargent.com) for clearly stating the goal of our analysis and\n",
    "determining the steps necessary to reach the goal.\n",
    "\n",
    "We always begin by writing `Want:` followed by what we want to\n",
    "accomplish.\n",
    "\n",
    "In this case, we would write:\n",
    "\n",
    "> Want: Count the number of months in which all states in our sample\n",
    "> had unemployment above 6.5%\n",
    "\n",
    "After identifying the **want**, we work *backwards* to identify the\n",
    "steps necessary to accomplish our goal.\n",
    "\n",
    "So, starting from the result, we have:\n",
    "\n",
    "1. Sum the number of `True` values in a Series indicating dates for\n",
    "   which all states had high unemployment.\n",
    "1. Build the Series used in the last step by using the `.all` method\n",
    "   on a DataFrame containing booleans indicating whether each state had\n",
    "   high unemployment at each date.\n",
    "1. Build the DataFrame used in the previous step using a `>`\n",
    "   comparison.\n",
    "\n",
    "Now that we have a clear plan, let's follow through and *apply* the want\n",
    "operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: construct the DataFrame of bools\n",
    "high = unemp > 6.5\n",
    "high.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ca28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: use the .all method on axis=1 to get the dates where all states have a True\n",
    "all_high = high.all(axis=1)\n",
    "all_high.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Call .sum to add up the number of True values in `all_high`\n",
    "#         (note that True == 1 and False == 0 in Python, so .sum will count Trues)\n",
    "msg = \"Out of {} months, {} had high unemployment across all states\"\n",
    "print(msg.format(len(all_high), all_high.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a362c1d",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: pd-bas-dir4\n",
    "See exercise 4 in the {ref}`exercise list <pd-bas-ex>`.\n",
    "````\n",
    "\n",
    "(pd-bas-ex)=\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Looking at the displayed DataFrame above, can you identify the index? The columns?\n",
    "\n",
    "You can use the cell below to verify your visual intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158d6a0",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-bas-dir1>`)\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Do the following exercises in separate code cells below:\n",
    "\n",
    "- At each date, what is the minimum unemployment rate across all states\n",
    "  in our sample?\n",
    "- What was the median unemployment rate in each state?\n",
    "- What was the maximum unemployment rate across the states in our\n",
    "  sample? What state did it happen in? In what month/year was this\n",
    "  achieved?\n",
    "    - ```{hint}\n",
    "      What Python type (not `dtype`) is returned by the aggregation?\n",
    "      ```\n",
    "    - ```{hint}\n",
    "      Read documentation for the method `idxmax`.\n",
    "      ```\n",
    "- Classify each state as high or low volatility based on whether the\n",
    "  variance of their unemployment is above or below 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min unemployment rate by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median unemployment rate by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max unemployment rate across all states and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low or high volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24f32a",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-bas-dir2>`)\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Imagine that we want to determine whether unemployment was high (> 6.5),\n",
    "medium (4.5 < x <= 6.5), or low (<= 4.5) for each state and each month.\n",
    "\n",
    "1. Write a Python function that takes a single number as an input and\n",
    "   outputs a single string noting if that number is high, medium, or low.\n",
    "1. Pass your function to `applymap` (quiz: why `applymap` and not\n",
    "   `agg` or `apply`?) and save the result in a new DataFrame called\n",
    "   `unemp_bins`.\n",
    "1. (Challenging) This exercise has multiple parts:\n",
    "    1. Use another transform on `unemp_bins` to count how many\n",
    "       times each state had each of the three classifications.\n",
    "        - ```{hint}\n",
    "          Will this value counting function be a Series or scalar transform?\n",
    "          ```\n",
    "        - ```{hint}\n",
    "          Try googling \"pandas count unique value\" or something similar to find the right transform.\n",
    "          ```\n",
    "    1. Construct a horizontal bar chart of the number of occurrences of\n",
    "       each level with one bar per state and classification (21 total\n",
    "       bars).\n",
    "1. (Challenging) Repeat the previous step, but count how many states had\n",
    "   each classification in each month. Which month had the most states\n",
    "   with high unemployment? What about medium and low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Write a Python function to classify unemployment levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Pass your function from part 1 to applymap\n",
    "unemp_bins = unemp.applymap#replace this comment with your code!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Count the number of times each state had each classification.\n",
    "\n",
    "## then make a horizontal bar chart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Apply the same transform from part 4, but to each date instead of to each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa4c97",
   "metadata": {},
   "source": [
    "({ref}`back to text <pd-bas-dir3>`)\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "- For a single state of your choice, determine what the mean\n",
    "  unemployment is during \"Low\", \"Medium\", and \"High\" unemployment times\n",
    "  (recall your `unemp_bins` DataFrame from the exercise above).\n",
    "    - Think about how you would do this for all the\n",
    "      states in our sample and write your thoughts... We will soon\n",
    "      learn tools that will *greatly* simplify operations like\n",
    "      this that operate on distinct *groups* of data at a time.\n",
    "- Which states in our sample performs the best during \"bad times?\" To\n",
    "  determine this, compute the mean unemployment for each state only for\n",
    "  months in which the mean unemployment rate in our sample is greater\n",
    "  than 7.\n",
    "\n",
    "({ref}`back to text <pd-bas-dir4>`)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   10,
   43,
   49,
   53,
   57,
   65,
   67,
   74,
   82,
   86,
   93,
   97,
   99,
   113,
   115,
   122,
   127,
   130,
   167,
   169,
   176,
   178,
   196,
   213,
   220,
   223,
   227,
   229,
   261,
   265,
   269,
   271,
   292,
   306,
   314,
   321,
   328,
   368,
   373,
   378,
   384,
   387,
   394,
   396,
   401,
   405,
   409,
   412,
   442,
   447,
   449,
   459,
   463,
   466,
   513,
   519,
   525,
   530,
   546,
   548,
   571,
   575,
   579,
   583,
   585,
   615,
   619,
   624,
   630,
   632
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}