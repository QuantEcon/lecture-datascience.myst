{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322e5056",
   "metadata": {},
   "source": [
    "# Working with Text\n",
    "\n",
    "**Author**\n",
    "> - [Paul Schrimpf *UBC*](https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/)\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- {doc}`Visualization Rules <visualization_rules>`\n",
    "- {doc}`Regression <regression>`\n",
    "- {doc}`Classification <classification>`\n",
    "- {doc}`Maps <maps>`\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Use text as features for classification\n",
    "- Understand latent topic analysis\n",
    "- Use folium to create an interactive map\n",
    "- Request and combine json data from a web server\n",
    "\n",
    "\n",
    "```{literalinclude} ../_static/colab_full.raw\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Many data sources contain both numerical data and text.\n",
    "\n",
    "We can use text to create features for any of the prediction methods\n",
    "that we have discussed.\n",
    "\n",
    "Doing so requires encoding text into some numerical representation.\n",
    "\n",
    "A good encoding preserves the meaning of the original text, while\n",
    "keeping dimensionality manageable.\n",
    "\n",
    "In this lecture, we will learn how to work with text through an\n",
    "application --- predicting fatalities from avalanche\n",
    "forecasts.\n",
    "\n",
    "## Avalanches\n",
    "\n",
    "Snow avalanches are a hazard in the mountains. Avalanches can be\n",
    "partially predicted based on snow conditions, weather, and\n",
    "terrain. [Avalanche Canada](https://www.avalanche.ca/map) produces\n",
    "daily avalanche forecasts for various Canadian mountainous regions.\n",
    "These forecasts consist of 1-5 ratings for each of three\n",
    "elevation bands, as well as textual descriptions of recent avalanche\n",
    "observations, snowpack, and weather. Avalanche Canada also\n",
    "maintains a list of [fatal avalanche incidents](https://www.avalanche.ca/incidents/) . In this lecture, we will\n",
    "attempt to predict fatal incidents from the text of avalanche\n",
    "forecasts. Since fatal incidents are rare, this prediction task will\n",
    "be quite difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c8810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45846170",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Avalanche Canada has an unstable json api. The api seems to be largely\n",
    "tailored to displaying the information on various Avalanche Canada\n",
    "websites, which does not make it easy to obtain large amounts of\n",
    "data. Nonetheless, getting information from the API is easier than\n",
    "scraping the website. Generally, whenever you're considering scraping\n",
    "a website, you should first check whether the site has an API available.\n",
    "\n",
    "#### Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b84bc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>location_province</th>\n",
       "      <th>group_activity</th>\n",
       "      <th>num_involved</th>\n",
       "      <th>num_injured</th>\n",
       "      <th>num_fatal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>645d13aa-4a96-4f11-a787-ded7a0051d63</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>Coppercrown Mountain</td>\n",
       "      <td>BC</td>\n",
       "      <td>Mechanized Skiing</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a764c492-bb8d-401d-b7b4-ec5d7915b98e</td>\n",
       "      <td>2023-02-16</td>\n",
       "      <td>Terminator 2.5</td>\n",
       "      <td>BC</td>\n",
       "      <td>Backcountry Skiing/Snowboarding</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222afb5-f434-48b9-a92b-19d472c3f914</td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>Potato Peak</td>\n",
       "      <td>BC</td>\n",
       "      <td>Backcountry Skiing</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8005ae1-313a-4a34-a327-16e6dadb19d6</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Akolkolex</td>\n",
       "      <td>BC</td>\n",
       "      <td>Mechanized Skiing</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd1d03e6-ed8e-4101-93e4-3c214d3d39ca</td>\n",
       "      <td>2023-01-21</td>\n",
       "      <td>Oasis</td>\n",
       "      <td>BC</td>\n",
       "      <td>Snowmobiling</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>101c517b-29a4-4c49-8934-f6c56ddd882d</td>\n",
       "      <td>1840-02-01</td>\n",
       "      <td>Château-Richer</td>\n",
       "      <td>QC</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>b2e1c50a-1533-4145-a1a2-0befca0154d5</td>\n",
       "      <td>1836-02-09</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>QC</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>18e8f963-da33-4682-9312-57ca2cc9ad8d</td>\n",
       "      <td>1833-05-24</td>\n",
       "      <td>Carbonear</td>\n",
       "      <td>NL</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>083d22df-ed50-4687-b9ab-1649960a0fbe</td>\n",
       "      <td>1825-02-04</td>\n",
       "      <td>Saint-Joseph de Lévis</td>\n",
       "      <td>QC</td>\n",
       "      <td>Inside Building</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>f498c48a-981d-43cf-ac16-151b8794435c</td>\n",
       "      <td>1782-01-01</td>\n",
       "      <td>Nain</td>\n",
       "      <td>NL</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id        date               location  \\\n",
       "0    645d13aa-4a96-4f11-a787-ded7a0051d63  2023-03-01   Coppercrown Mountain   \n",
       "1    a764c492-bb8d-401d-b7b4-ec5d7915b98e  2023-02-16         Terminator 2.5   \n",
       "2    1222afb5-f434-48b9-a92b-19d472c3f914  2023-02-11            Potato Peak   \n",
       "3    e8005ae1-313a-4a34-a327-16e6dadb19d6  2023-01-23              Akolkolex   \n",
       "4    bd1d03e6-ed8e-4101-93e4-3c214d3d39ca  2023-01-21                  Oasis   \n",
       "..                                    ...         ...                    ...   \n",
       "495  101c517b-29a4-4c49-8934-f6c56ddd882d  1840-02-01         Château-Richer   \n",
       "496  b2e1c50a-1533-4145-a1a2-0befca0154d5  1836-02-09                 Quebec   \n",
       "497  18e8f963-da33-4682-9312-57ca2cc9ad8d  1833-05-24              Carbonear   \n",
       "498  083d22df-ed50-4687-b9ab-1649960a0fbe  1825-02-04  Saint-Joseph de Lévis   \n",
       "499  f498c48a-981d-43cf-ac16-151b8794435c  1782-01-01                   Nain   \n",
       "\n",
       "    location_province                   group_activity  num_involved  \\\n",
       "0                  BC                Mechanized Skiing          10.0   \n",
       "1                  BC  Backcountry Skiing/Snowboarding           6.0   \n",
       "2                  BC               Backcountry Skiing           2.0   \n",
       "3                  BC                Mechanized Skiing           3.0   \n",
       "4                  BC                     Snowmobiling           2.0   \n",
       "..                ...                              ...           ...   \n",
       "495                QC                          Unknown           NaN   \n",
       "496                QC                          Unknown           NaN   \n",
       "497                NL                          Unknown           NaN   \n",
       "498                QC                  Inside Building           NaN   \n",
       "499                NL                          Unknown           NaN   \n",
       "\n",
       "     num_injured  num_fatal  \n",
       "0            4.0          3  \n",
       "1            1.0          2  \n",
       "2            0.0          2  \n",
       "3            1.0          2  \n",
       "4            0.0          1  \n",
       "..           ...        ...  \n",
       "495          NaN          1  \n",
       "496          NaN          1  \n",
       "497          0.0          1  \n",
       "498          NaN          5  \n",
       "499          NaN         22  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data on avalanche forecasts and incidents from Avalanche Canada\n",
    "# Avalanche Canada has an unstable public api\n",
    "# https://github.com/avalanche-canada/ac-web\n",
    "# Since API might change, this code might break\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Incidents\n",
    "url = \"http://incidents.avalanche.ca/public/incidents/?format=json\"\n",
    "req = urllib.request.Request(url)\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    result = json.loads(response.read().decode('utf-8'))\n",
    "incident_list = result[\"results\"]\n",
    "while (result[\"next\"] != None):\n",
    "    req = urllib.request.Request(result[\"next\"])\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    incident_list = incident_list + result[\"results\"]\n",
    "incidents_brief = pd.DataFrame.from_dict(incident_list,orient=\"columns\")\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8\n",
    "incidents_brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db76951e",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     incident_detail_list \u001b[38;5;241m=\u001b[39m incidents_brief\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mapply(get_incident_details)\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m     16\u001b[0m     incidents \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(incident_detail_list, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mincidents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincidentsfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     incidents \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(incidentsfile)\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/pandas/io/common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/urllib/request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# We can get more information about these incidents e.g. \"https://www.avalanche.ca/incidents/37d909e4-c6de-43f1-8416-57a34cd48255\"\n",
    "# this information is also available through the API\n",
    "def get_incident_details(id):\n",
    "    url = \"http://incidents.avalanche.ca/public/incidents/{}?format=json\".format(id)\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    return(result)\n",
    "\n",
    "\n",
    "incidentsfile = \"https://datascience.quantecon.org/assets/data/avalanche_incidents.csv\"\n",
    "\n",
    "# To avoid loading the avalanche Canada servers, we save the incident details locally.\n",
    "if (not os.path.isfile(incidentsfile)):\n",
    "    incident_detail_list = incidents_brief.id.apply(get_incident_details).to_list()\n",
    "    incidents = pd.DataFrame.from_dict(incident_detail_list, orient=\"columns\")\n",
    "    incidents.to_csv(incidentsfile)\n",
    "else:\n",
    "    incidents = pd.read_csv(incidentsfile)\n",
    "\n",
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42deb11a",
   "metadata": {},
   "source": [
    "Many incidents include coordinates, but others do not. Most\n",
    "however, do include a place name. We can use [Natural Resources Canada's\n",
    "Geolocation Service](https://www.nrcan.gc.ca/earth-sciences/geography/topographic-information/geolocalisation-service/17304)\n",
    "to retrieve coordinates from place names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c39fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode locations without coordinates\n",
    "def geolocate(location, province):\n",
    "    url = \"http://geogratis.gc.ca/services/geolocation/en/locate?q={},%20{}\"\n",
    "    req = urllib.request.Request(url.format(urllib.parse.quote(location),province))\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    if (len(result)==0):\n",
    "        return([None,None])\n",
    "    else:\n",
    "        return(result[0]['geometry']['coordinates'])\n",
    "if not \"alt_coord\" in incidents.columns:\n",
    "    incidents[\"alt_coord\"] = [\n",
    "        geolocate(incidents.location[i], incidents.location_province[i])\n",
    "        for i in incidents.index\n",
    "    ]\n",
    "    incidents.to_csv(incidentsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d899664",
   "metadata": {},
   "source": [
    "Now that we have incident data, let's create some figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up activity names\n",
    "incidents.group_activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bcb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.group_activity=incidents.group_activity.replace(\"Ski touring\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Out-of-Bounds Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Lift Skiing Closed\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing and Hiking\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Mechanized Skiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Heliskiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"At Outdoor Worksite\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Control Work\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Hunting/Fishing\",\"Other Recreational\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Building\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Outside Building\",\"Car/Truck/Building\")\n",
    "\n",
    "\n",
    "incidents.group_activity.unique()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "incidents.groupby(['group_activity']).id.count().plot(kind='bar', title=\"Incidents by Activity\", ax=ax[0])\n",
    "incidents.groupby(['group_activity']).num_fatal.sum().plot(kind='bar', title=\"Deaths by Activity\", ax=ax[1], color=colors[1])\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_xlabel(None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78659c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents[\"date\"] = pd.to_datetime(incidents.ob_date)\n",
    "incidents[\"year\"] = incidents.date.apply(lambda x: x.year)\n",
    "incidents.date = incidents.date.dt.date\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "f = incidents.groupby([\"year\"]).num_fatal.sum()\n",
    "n = incidents.groupby([\"year\"]).id.count()\n",
    "yearstart=1950\n",
    "f=f[f.index>yearstart]\n",
    "n=n[n.index>yearstart]\n",
    "fig,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "n.plot(ax=ax)\n",
    "f.plot(ax=ax)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.annotate(\"Incidents\", (2010, 4), color=colors[0])\n",
    "ax.annotate(\"Deaths\", (2011, 15), color=colors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71104d94",
   "metadata": {},
   "source": [
    "#### Mapping Incidents\n",
    "\n",
    "Since the incident data includes coordinates, we might as well make a\n",
    "map too. Unfortunately, some latitude and longitudes contain obvious errors.\n",
    "Here, we try to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55664b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# fix errors in latitude, longitude\n",
    "latlon = incidents.location_coords\n",
    "def makenumeric(cstr):\n",
    "    if cstr is None:\n",
    "        return([None,None])\n",
    "    elif (type(cstr)==str):\n",
    "        return([float(s) for s in re.findall(r'-?\\d+\\.?\\d*',cstr)])\n",
    "    else:\n",
    "        return(cstr)\n",
    "\n",
    "latlon = latlon.apply(makenumeric)\n",
    "\n",
    "def good_lat(lat):\n",
    "    return(lat >= 41.6 and lat <= 83.12) # min & max for Canada\n",
    "\n",
    "def good_lon(lon):\n",
    "    return(lon >= -141 and lon<= -52.6)\n",
    "\n",
    "def fixlatlon(c):\n",
    "    if (len(c)<2 or type(c[0])!=float or type(c[1])!=float):\n",
    "        c = [None, None]\n",
    "        return(c)\n",
    "    lat = c[0]\n",
    "    lon = c[1]\n",
    "    if not good_lat(lat) and good_lat(lon):\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lon):\n",
    "        lon = -lon\n",
    "    if not good_lon(lon) and good_lon(lat):\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lat):\n",
    "        tmp = -lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lat(lat) or not good_lon(lon):\n",
    "        c[0] = None\n",
    "        c[1] = None\n",
    "    else:\n",
    "        c[0] = lat\n",
    "        c[1] = lon\n",
    "    return(c)\n",
    "\n",
    "incidents[\"latlon\"] = latlon.apply(fixlatlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f59ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(c, a):\n",
    "    if (type(a)==str):\n",
    "        a = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*',a)]\n",
    "    if len(a) <2:\n",
    "        a = [None,None]\n",
    "    return([a[1],a[0]] if type(c[0])!=float else c)\n",
    "incidents[\"latlon_filled\"]=[foo(c,a) for c,a in zip(incidents[\"latlon\"],incidents[\"alt_coord\"])]\n",
    "nmiss = sum([a[0]==None for a in incidents.latlon_filled])\n",
    "n = len(incidents.latlon_filled)\n",
    "print(\"{} of {} incidents have latitude & longitude\".format(n-nmiss, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download forecast region definitions\n",
    "# req = urllib.request.Request(\"https://www.avalanche.ca/api/forecasts\")\n",
    "# The above link doesn't work since COVID-19 lockdown. Currently we use an old cached version instead\n",
    "#req = (\"https://web.archive.org/web/20150319031605if_/http://www.avalanche.ca/api/forecasts\")\n",
    "#with urllib.request.urlopen(req) as response:\n",
    "#    forecastregions = json.loads(response.read().decode('utf-8'))\n",
    "req = \"https://faculty.arts.ubc.ca/pschrimpf/forecast-regions2015.json\"\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions2015 = json.loads(response.read().decode('utf-8'))\n",
    "\n",
    "req = \"https://faculty.arts.ubc.ca/pschrimpf/forecast-regions2019.json\"\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions2019 = json.loads(response.read().decode('utf-8'))\n",
    "\n",
    "forecastregions = regions2019\n",
    "ids = [r['id'] for r in forecastregions['features']]\n",
    "for r in regions2015['features'] :\n",
    "     if not r['id'] in ids :\n",
    "            forecastregions['features'].append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e768f94",
   "metadata": {},
   "source": [
    "You may have to uncomment the second line below if  folium is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b114573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map forecast regions and incidents\n",
    "#!pip install --user folium\n",
    "import folium\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "fmap = folium.Map(location=[60, -98],\n",
    "                            zoom_start=3,\n",
    "                            tiles='Stamen Terrain')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions_tmp = json.loads(response.read().decode('utf-8'))\n",
    "folium.GeoJson(regions_tmp,\n",
    "               tooltip=folium.GeoJsonTooltip(fields=[\"name\"], aliases=[\"\"]),\n",
    "               highlight_function=lambda x: { 'weight': 10},\n",
    "              style_function=lambda x: {'weight':1}).add_to(fmap)\n",
    "activities = incidents.group_activity.unique()\n",
    "for i in incidents.index:\n",
    "    if incidents.latlon_filled[i][0] is not None and  incidents.latlon_filled[i][1] is not None:\n",
    "        cindex=[j for j,x in enumerate(activities) if x==incidents.group_activity[i]][0]\n",
    "        txt = \"{}, {}<br>{} deaths\"\n",
    "        txt = txt.format(incidents.group_activity[i],\n",
    "                        incidents.ob_date[i],\n",
    "                        incidents.num_fatal[i]\n",
    "                        )\n",
    "        pop = folium.Popup(incidents.comment[i], parse_html=True, max_width=400)\n",
    "        folium.CircleMarker(incidents.latlon_filled[i],\n",
    "                      tooltip=txt,\n",
    "                      popup=pop,\n",
    "                      color=matplotlib.colors.to_hex(cmap(cindex)), fill=True, radius=5).add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5dd88b",
   "metadata": {},
   "source": [
    "Take a moment to click around the map and read about some of the incidents.\n",
    "\n",
    "Between presenting this information on a map and the list on [https://www.avalanche.ca/incidents/](https://www.avalanche.ca/incidents/) ,\n",
    "which do you prefer and why?\n",
    "\n",
    "#### Matching Incidents to Regions\n",
    "\n",
    "Later, we will want to match incidents to forecasts, so let's find the closest region to each incident.\n",
    "\n",
    "Note that distance here will be in units of latitude, longitude (or\n",
    "whatever coordinate system we use). At the equator, a distance of 1 is\n",
    "approximately 60 nautical miles.\n",
    "\n",
    "Since longitude lines get closer together farther from the equator,\n",
    "these distances will be understated the further North you go.\n",
    "\n",
    "This is not much of a problem if we're just finding the\n",
    "nearest region, but if we care about accurate distances, we should\n",
    "re-project the latitude and longitude into a different coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match incidents to nearest forecast regions.\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "point = Point(incidents.latlon_filled[0][1],incidents.latlon_filled[0][0])\n",
    "def distances(latlon):\n",
    "    point=Point(latlon[1],latlon[0])\n",
    "    df = pd.DataFrame.from_dict([{'id':feature['id'],\n",
    "                                  'distance':shape(feature['geometry']).distance(point)} for\n",
    "                                 feature in forecastregions['features']])\n",
    "    return(df)\n",
    "def foo(x):\n",
    "    if (x[0]==None):\n",
    "        return(None)\n",
    "    d = distances(x)\n",
    "    return(d.id[d.distance.idxmin()])\n",
    "incidents['nearest_region'] = incidents.latlon_filled.apply(foo)\n",
    "incidents['nearest_distance'] = incidents.latlon_filled.apply(lambda x: None if x[0]==None else distances(x).distance.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56caf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26903d16",
   "metadata": {},
   "source": [
    "#### Forecast Data\n",
    "\n",
    "We'll now download all forecasts for all regions since November 2011 (roughly the earliest data available).\n",
    "\n",
    "We can only request one forecast at a time, so this takes many hours to download.\n",
    "\n",
    "To make this process run more quickly for readers, we ran the code ourselves and then stored the data in the cloud.\n",
    "\n",
    "The function below will fetch all the forecasts from the cloud storage location and save them to a folder\n",
    "named `avalanche_forecasts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb880a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cached_forecasts():\n",
    "    # download the zipped file and unzip it here\n",
    "    url = \"https://datascience.quantecon.org/assets/data/avalanche_forecasts.zip?raw=true\"\n",
    "    with requests.get(url) as res:\n",
    "        if not res.ok:\n",
    "            raise ValueError(\"failed to download the cached forecasts\")\n",
    "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
    "            for f in z.namelist():\n",
    "                if (os.path.isfile(f) and z.getinfo(f).file_size < os.stat(f).st_size):\n",
    "                    warnings.warn(f\"'File $f exists and is larger than version in cache. Not replacing.\")\n",
    "                else :\n",
    "                    z.extract(f)\n",
    "\n",
    "download_cached_forecasts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2080e",
   "metadata": {},
   "source": [
    "The code below is what we initially ran to obtain all the forecasts.\n",
    "\n",
    "You will notice that this code checks to see whether the files can be found in the `avalanche_forecasts`\n",
    "directory (they can if you ran the `download_cached_forecasts` above!) and will only download them if they aren't found.\n",
    "\n",
    "You can experiment with this caching by deleting one or more files from the `avalanche_forecasts`\n",
    "folder and re-running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for downloading forecasts from Avalanche Canada\n",
    "\n",
    "def get_forecast(date, region):\n",
    "    url = \"https://www.avalanche.ca/api/bulletin-archive/{}/{}.json\".format(date.isoformat(),region)\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            result = json.loads(response.read().decode('utf-8'))\n",
    "        return(result)\n",
    "    except:\n",
    "        return(None)\n",
    "\n",
    "def get_forecasts(start, end, region):\n",
    "    day = start\n",
    "    forecasts = []\n",
    "    while(day<=end and day<end.today()):\n",
    "        #print(\"working on {}, {}\".format(region,day))\n",
    "        forecasts = forecasts + [get_forecast(day, region)]\n",
    "        #print(\"sleeping\")\n",
    "        time.sleep(0.1) # to avoid too much load on Avalanche Canada servers\n",
    "        day = day + pd.Timedelta(1,\"D\")\n",
    "    return(forecasts)\n",
    "\n",
    "def get_season(year, region):\n",
    "    start_month = 11\n",
    "    start_day = 20\n",
    "    last_month = 5\n",
    "    last_day = 1\n",
    "    if (not os.path.isdir(\"avalanche_forecasts\")):\n",
    "        os.mkdir(\"avalanche_forecasts\")\n",
    "    seasonfile = \"avalanche_forecasts/{}_{}-{}.json\".format(region, year, year+1)\n",
    "    if (not os.path.isfile(seasonfile)):\n",
    "        startdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year, start_month, start_day))\n",
    "        lastdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year+1, last_month, last_day))\n",
    "        season = get_forecasts(startdate,lastdate,region)\n",
    "        with open(seasonfile, 'w') as outfile:\n",
    "            json.dump(season, outfile, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(seasonfile, \"rb\") as json_data:\n",
    "            season = json.load(json_data)\n",
    "    return(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastlist=[]\n",
    "\n",
    "for year in range(2011,2019):\n",
    "    print(\"working on {}\".format(year))\n",
    "    for region in [region[\"id\"] for region in forecastregions[\"features\"]]:\n",
    "        forecastlist = forecastlist + get_season(year, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame and extract some variables\n",
    "forecasts = pd.DataFrame.from_dict([f for f in forecastlist if not f==None],orient=\"columns\")\n",
    "\n",
    "forecasts[\"danger_date\"] = forecasts.dangerRatings.apply(lambda r: r[0][\"date\"])\n",
    "forecasts[\"danger_date\"] = pd.to_datetime(forecasts.danger_date, utc=True).dt.date\n",
    "forecasts[\"danger_alpine\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"alp\"])\n",
    "forecasts[\"danger_treeline\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"tln\"])\n",
    "forecasts[\"danger_belowtree\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"btl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge incidents to forecasts\n",
    "adf = pd.merge(forecasts, incidents, how=\"left\",\n",
    "               left_on=[\"region\",\"danger_date\"],\n",
    "               right_on=[\"nearest_region\",\"date\"],\n",
    "              indicator=True)\n",
    "adf[\"incident\"] = adf._merge==\"both\"\n",
    "print(\"There were {} incidents matched with forecasts data. These occured on {}% of day-regions with forecasts\".format(adf.incident.sum(),adf.incident.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11346080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ratings=sorted(adf.danger_alpine.unique())\n",
    "ava_colors = [\"#52BA4A\", \"#FFF300\", \"#F79218\", \"#EF1C29\", \"#1A1A1A\", \"#BFBFBF\"]\n",
    "for x in [\"danger_alpine\", \"danger_treeline\", \"danger_belowtree\"]:\n",
    "    fig=sns.catplot(x=x, kind=\"count\",col=\"incident\", order=ratings, data=adf, sharey=False,\n",
    "                    palette=ava_colors, height=3, aspect=2)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    fig.fig.suptitle(x.replace(\"danger_\",\"\"))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c455909",
   "metadata": {},
   "source": [
    "## Predicting Incidents from Text\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The first step when using text as data is to pre-process the text.\n",
    "\n",
    "In preprocessing, we will:\n",
    "\n",
    "1. Clean: Remove unwanted punctuation and non-text characters.\n",
    "1. Tokenize: Break sentences down into words.\n",
    "1. Remove \"stopwords\": Eliminate common words that actually provide no information, like \"a\" and \"the\".\n",
    "1. Lemmatize words: Reduce words to their dictionary \"lemma\" e.g. \"snowing\" and \"snowed\" both become snow (verb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Remove stopwords (the, a, is, etc)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords=stopwords.union(set(string.punctuation))\n",
    "# Lemmatize words e.g. snowed and snowing are both snow (verb)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "def text_prep(txt):\n",
    "    soup = BeautifulSoup(txt, \"lxml\")\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    txt=soup.text # remove html tags\n",
    "    txt = txt.lower()\n",
    "    tokens = [token for token in nltk.tokenize.word_tokenize(txt)]\n",
    "    tokens = [token for token in tokens if not token in stopwords]\n",
    "    #tokens = [token for token in tokens if not token ]\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "    if (len(tokens)==0):\n",
    "        tokens = [\"EMPTYSTRING\"]\n",
    "    return(tokens)\n",
    "\n",
    "text_prep(forecasts.highlights[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66e5f4",
   "metadata": {},
   "source": [
    "Now, let's apply this to all avalanche summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075168d8",
   "metadata": {},
   "source": [
    "Let's make a bar plot of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = nltk.FreqDist([word for doc in text_data for word in doc]).most_common(20)\n",
    "words = [x[0] for x in wf]\n",
    "cnt = [x[1] for x in wf]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.bar(range(len(words)), cnt);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Most common words in avalanche summaries');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e92b0",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The \"bag of words\" approach is the simplest way to convert a collection of processed text\n",
    "documents to a feature matrix. We view\n",
    "each document as a bag of words, and our feature matrix\n",
    "counts how many times each word appears. This method is called a \"bag of words\"\n",
    "because we ignore the document's word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b461b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]\n",
    "y = adf.incident\n",
    "X = vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bdf8f0",
   "metadata": {},
   "source": [
    "We can also perform more complicated feature engineering. One extension of the \"bag of words\" method\n",
    "is to consider counts of pairs or triples of consecutive words. These\n",
    "are called n-grams and can be created by setting the `n_gram`\n",
    "argument to `CountVectorizer`. Another alternative might be to accommodate\n",
    "the fact that common words will inherently have higher counts by using\n",
    "term-frequency inverse-document-frequency (see below).\n",
    "\n",
    "After creating our feature matrix, we can now apply any classification\n",
    "method to predict incidents.\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "A common text data classifier is the Naive Bayes classifier.\n",
    "This classifier predicts incidents using Bayes' rules.\n",
    "\n",
    "$$\n",
    "P(incident | words) = \\frac{P(words|incident) P(incidents)}{P(words)}\n",
    "$$\n",
    "\n",
    "The classifier is naive, though; it assumes words are independent of one another in any given incident.\n",
    "\n",
    "$$\n",
    "P(words|incident) = \\prod_{w \\in words} P(w|incident)\n",
    "$$\n",
    "\n",
    "Although this assumption is implausible for text, the Naive Bayes\n",
    "classifier can be computed extremely quickly, and sometimes quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53411cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(Xtrain,ytrain)\n",
    "np.mean(classifier.predict(Xtest)==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b772be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(ytest, classifier.predict(Xtest)))\n",
    "print(metrics.classification_report(ytest, classifier.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print text with highest predicted probabilities\n",
    "phat=classifier.predict_proba(X)[:,1]\n",
    "def remove_html(txt):\n",
    "    soup = BeautifulSoup(txt, \"lxml\")\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    return(soup.text)\n",
    "docs = [remove_html(txt) for txt in adf.avalancheSummary]\n",
    "txt_high = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0],reverse=True)]\n",
    "txt_high[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print text with lowest predicted probabilities\n",
    "txt_low = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0])]\n",
    "txt_low[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b2349",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: app-txt-dir1\n",
    "\n",
    "See exercise 1 in the {ref}`exercise list <app-txt-ex>`.\n",
    "````\n",
    "\n",
    "Predicting deaths from forecast text is very difficult because deaths\n",
    "are so rare. A prediction exercise more likely to succeed would be to\n",
    "predict the avalanche rating from the forecast text. However,\n",
    "doing so is a very\n",
    "artificial task, with little practical use.\n",
    "\n",
    "Another alternative would be to gather more data on non-fatal\n",
    "avalanches. Avalanche Canada also has user-submitted \"Mountain\n",
    "Information Network\" reports. These reports include observations of\n",
    "natural avalanches and information on non-fatal avalanche\n",
    "incidents. Since the data is user-submitted, it is messy and more\n",
    "difficult to work with. Nonetheless, working with it would be\n",
    "good practice and could lead to some insights.\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "The regression and classification methods that we have seen so far are\n",
    "examples of supervised learning --- we are trying to predict an observed outcome.\n",
    "In unsupervised learning, we do not have an\n",
    "observed outcome to predict. Instead, we try to find informative\n",
    "patterns in the data. Unsupervised learning can be particularly useful\n",
    "with text data. We will look at two related techniques for topic\n",
    "modeling. These techniques attempt to extract distinct topics from a\n",
    "collection of text documents.\n",
    "\n",
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent semantic analysis is used by some search engines to rank\n",
    "the similarities among documents. Latent semantic analysis begins with a\n",
    "term document matrix, $X$. The term document matrix is a number\n",
    "of documents by number of terms matrix where the i,jth entry is the\n",
    "measure of how often term j appears in document i. This could be the\n",
    "same bag of words feature matrix we constructed above, or it could be\n",
    "some other measure. For this example, we will use the term-frequency,\n",
    "inverse-document-frequency representation.\n",
    "\n",
    "$$\n",
    "x^{tfidf}_{ij} = \\frac{\\text{occurences of term j in document\n",
    "i}}{\\text{length of document i}} \\log \\left(\\frac{\\text{number of\n",
    "documents}}{\\text{number of documents containing term j}}\\right)\n",
    "$$\n",
    "\n",
    "Given a term document matrix, $X$, latent semantic analysis\n",
    "computes a lower rank approximation to $X$ through the singular\n",
    "value decomposition. This lower rank approximation can potentially be\n",
    "interpreted or used instead of $X$ for other learning\n",
    "algorithms. In other contexts, similar decompositions are referred to\n",
    "as principal components analysis or factor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb01ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a33de",
   "metadata": {},
   "source": [
    "Here, we have computed a rank 10 approximation to the tf-idf matrix. We\n",
    "can see how much variance of the original matrix that our 10\n",
    "components reproduce. We can also look at how all terms in the\n",
    "document contribute to each of the 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd_model.explained_variance_ratio_)\n",
    "print(svd_model.explained_variance_ratio_.cumsum())\n",
    "terms = tfidf_vectorizer.get_feature_names_out() \n",
    "comp_label=[]\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    message = \"\"\n",
    "    for t in sorted_terms:\n",
    "        message = message + \"{:.2f} * {} + \".format(t[1],t[0])\n",
    "    print(message)\n",
    "    comp_label.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177adff",
   "metadata": {},
   "source": [
    "Finally, we can attempt to visualize the components.\n",
    "\n",
    "The LSA reduced the dimensionality of the representation of documents\n",
    "from thousands (of term-frequency inverse-document-frequency) to ten\n",
    "components. While ten is more manageable than thousands, it is still too\n",
    "many dimensions to effectively visualize. t-SNE is a technique to further\n",
    "reduce dimensionality. t-SNE is a nonlinear data transformation\n",
    "from many dimensions to 2, while attempting to preserve the\n",
    "original clustering of their original domain. In other words, if a\n",
    "set of documents are closely clustered in the 10 dimensional\n",
    "LSA space, then they will also be close together in the 2 dimensional\n",
    "t-SNE representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_topic_matrix = svd_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "nplot = 2000 # reduce the size of the data to speed computation and make the plot less cluttered\n",
    "lsa_topic_sample = lsa_topic_matrix[np.random.choice(lsa_topic_matrix.shape[0], nplot, replace=False)]\n",
    "tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=500,\n",
    "                      n_iter=1000, verbose=10, random_state=0, angle=0.75)\n",
    "tsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44614542",
   "metadata": {},
   "source": [
    "The t-SNE model creates a\n",
    "non-linear projection from our 10 dimensional LSA topics onto two dimensional space.\n",
    "It can be useful for visualizing high-dimensional data. One word of caution:\n",
    "the output of the t-SNE model can depend on the parameters of the\n",
    "algorithm. Failure to see clear clusters in the t-SNE visualization\n",
    "could mean either the original data was not clustered in higher\n",
    "dimensional space or that the t-SNE algorithm parameters were\n",
    "chosen poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('Paired')\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "n_topics=len(svd_model.components_)\n",
    "lsa_keys = np.argmax(lsa_topic_sample, axis=1)\n",
    "ax[0].scatter(x=tsne_lsa_vectors[:,0],y=tsne_lsa_vectors[:,1], color=[cmap(i) for i in lsa_keys], alpha=0.8)\n",
    "bbox_props = dict(boxstyle=\"round4,pad=0.1\", lw=0.2, fc=\"white\")\n",
    "for i in range(n_topics):\n",
    "    m = tsne_lsa_vectors[lsa_keys==i, :].mean(axis=0)\n",
    "    ax[0].text(m[0], m[1], str(i), ha=\"center\", va=\"center\",\n",
    "               size=15, color=cmap(i),\n",
    "               bbox=bbox_props)\n",
    "    ax[1].text(0,1-(i+1)*1/(n_topics+1),\"Topic \" + str(i) + \" : \"+ comp_label[i],ha=\"left\", va=\"center\", color=cmap(i))\n",
    "    ax[1].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ba17a",
   "metadata": {},
   "source": [
    "From this plot, we can immediately see two things. First, most documents\n",
    "are closest to topic 0. Second, most topics are not\n",
    "well-separated.\n",
    "\n",
    "````{admonition} Exercise\n",
    ":name: app-txt-dir2\n",
    "\n",
    "See exercise 2 in the {ref}`exercise list <app-txt-ex>`.\n",
    "````\n",
    "\n",
    "### Latent Dirichlet Analysis\n",
    "\n",
    "Latent dirichlet analysis (LDA) produces similar outputs as latent semantic\n",
    "analysis, but LDA often produces nicer results. The statistical theory\n",
    "underlying LSA is built on continuous $X$ features. LDA uses\n",
    "similar ideas, but takes into account that text is discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afeb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "# gensim works with a list of lists of tokens\n",
    "text_data = [text_prep(txt) for txt in forecasts.avalancheSummary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bag of words\n",
    "dictionary = gensim.corpora.Dictionary(text_data)\n",
    "bow_data = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(bow_data, num_topics = 5, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = pyLDAvis.gensim_models.prepare(ldamodel, bow_data, dictionary)\n",
    "lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0b1e8",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":name: app-txt-dir3\n",
    "\n",
    "See exercise 3 in the {ref}`exercise list <app-txt-ex>`.\n",
    "````\n",
    "\n",
    "````{admonition} Exercise\n",
    ":name: app-txt-dir4\n",
    "\n",
    "See exercise 4 in the {ref}`exercise list <app-txt-ex>`.\n",
    "````\n",
    "\n",
    "(app-txt-ex)=\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Use another classification method to predict incidents. Check whether\n",
    "your method outperforms the Naive Bayes classifier.\n",
    "\n",
    "({ref}`back to text <app-txt-dir1>`)\n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Apply LSA to the weather or snowpack descriptions. Can you notice\n",
    "any patterns?\n",
    "\n",
    "({ref}`back to text <app-txt-dir2>`)\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Apply LDA to the weather or snowpack descriptions. Can you notice\n",
    "any patterns?\n",
    "\n",
    "({ref}`back to text <app-txt-dir3>`)\n",
    "\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Use the reduced rank representation of text from LSA or LDA as a\n",
    "feature matrix to predict avalanche incidents. Compare the\n",
    "performance with the bag of words feature matrix.\n",
    "\n",
    "({ref}`back to text <app-txt-dir4>`)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   10,
   65,
   72,
   85,
   118,
   140,
   147,
   164,
   168,
   173,
   201,
   217,
   225,
   277,
   290,
   310,
   314,
   345,
   367,
   386,
   388,
   401,
   416,
   426,
   470,
   479,
   490,
   494,
   504,
   514,
   529,
   556,
   560,
   562,
   566,
   579,
   589,
   595,
   625,
   630,
   637,
   643,
   655,
   659,
   716,
   724,
   727,
   734,
   748,
   763,
   767,
   774,
   785,
   800,
   819,
   826,
   832,
   839,
   845
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}