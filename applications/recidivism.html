

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Case Study: Recidivism &#8212; QuantEcon DataScience</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/quantecon-book-theme.279dae03c5caae754d20501e3fa00bbf.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />


    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-S8CBQPC844"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-S8CBQPC844');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"extensions": ["autobold.js"], "macros": {"argmax": "arg\\,max", "argmin": "arg\\,min", "col": "col"}, "processEscapes": true}, "svg": {"scale": "0.92,"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'applications/recidivism';</script>
    <link rel="canonical" href="https://datascience.quantecon.org/applications/recidivism.html" />
    <link rel="shortcut icon" href="../_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Working with Text" href="working_with_text.html" />
    <link rel="prev" title="Social and Economic Networks" href="networks.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Chase Coleman, Spencer Lyon, and Jesse Perla" />
<meta name="keywords" content="Python, QuantEcon, DataScience" />
<meta name="description" content=This website presents a series of lectures on programming, data science, and economics. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Case Study: Recidivism"/>
<meta name="twitter:description" content="This website presents a series of lectures on programming, data science, and economics.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Case Study: Recidivism" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://datascience.quantecon.org/applications/recidivism.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a series of lectures on programming, data science, and economics." />
<meta property="og:site_name" content="QuantEcon DataScience" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=applications/recidivism>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-recidivism">Introduction to Recidivism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-description">Data Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#descriptive-statistics">Descriptive Statistics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#age-sex-and-race">Age, Sex, and Race</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recidivism">Recidivism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#risk-scores">Risk Scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#risk-scores-and-recidivism">Risk Scores and Recidivism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">Preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-compas-scores">Predicting COMPAS Scores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-probability-models">Binary Probability Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#false-positive-and-negative-rates">False Positive and Negative Rates</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-calibration-and-balance">Visualizing Calibration and Balance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-alternative-prediction">Creating an Alternative Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizing-to-maximize-balance">Regularizing to Maximize Balance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tradeoffs-are-inevitable">Tradeoffs are Inevitable</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3">Exercise 3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4">Exercise 4</a></li>
</ul>
</li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="../_static/datascience-logo.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="../index.html">QuantEcon DataScience</a></p>

                        <p class="qe-page__header-subheading">Case Study: Recidivism</p>

                    </div>

                    <p class="qe-page__header-authors">Chase Coleman, Spencer Lyon, and Jesse Perla</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="case-study-recidivism">
<h1>Case Study: Recidivism<a class="headerlink" href="#case-study-recidivism" title="Permalink to this heading">#</a></h1>
<p><strong>Co-authors</strong></p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/">Paul Schrimpf <em>UBC</em></a></p></li>
<li><p><a class="reference external" href="https://arnavsood.com">Arnav Sood <em>UBC</em></a></p></li>
</ul>
</div></blockquote>
<p><strong>Prerequisites</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../tools/matplotlib.html"><span class="doc">matplotlib Introduction</span></a></p></li>
<li><p><a class="reference internal" href="../tools/visualization_rules.html"><span class="doc">Visualization Rules</span></a></p></li>
<li><p><a class="reference internal" href="../tools/regression.html"><span class="doc">Regression</span></a></p></li>
</ul>
<p><strong>Outcomes</strong></p>
<ul class="simple">
<li><p>See an end-to-end data science exercise</p></li>
<li><p>Application of regression</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment following line to install on colab</span>
<span class="c1">#! pip install fiona geopandas xgboost gensim folium pyLDAvis descartes</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">linear_model</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">neural_network</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">model_selection</span>
<span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<section id="introduction-to-recidivism">
<h2>Introduction to Recidivism<a class="headerlink" href="#introduction-to-recidivism" title="Permalink to this heading">#</a></h2>
<p>Recidivism is the tendency for an individual who has previously committed a crime to commit another crime
in the future.</p>
<p>One key input to a judge’s sentencing decision is how likely a given convict is to re-offend, or recidivate.</p>
<p>In an effort to assist the legal system with sentencing guidelines, data scientists have attempted
to predict an individual’s risk of recidivism from known observables.</p>
<p>Some are concerned that this process may exhibit prejudice, either through biased inputs
or through statistical discrimination.</p>
<p>For example,</p>
<ol class="arabic simple">
<li><p>Biased inputs: Imagine that a judge often writes harsher sentences to people of a particular race or gender. If an algorithm is trained to reproduce the sentences of this judge, the bias will be propagated by the algorithm.</p></li>
<li><p>Statistical discrimination: Imagine that two variables (say race and income) are correlated, and one of them (say income) is correlated with the risk of recidivism. If income is unobserved, then an otherwise unbiased method would discriminate based on race, even if race has nothing to say about recidivism after controlling for income.</p></li>
</ol>
<p>This has given rise to serious discussions about the moral obligations data scientists have to
those who are affected by their tools.</p>
<p>We will not take a stance today on our moral obligations, but we believe this is an important
precursor to any statistical work with public policy applications.</p>
<p>One predictive tool used by various courts in the United States is
called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions).</p>
<p>We will be following a <a class="reference external" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Pro Publica</a>
article that analyzes the output of COMPAS.</p>
<p>The findings of the article include:</p>
<ul class="simple">
<li><p>Black defendants were often predicted to be at a higher risk of recidivism than they actually were.</p></li>
<li><p>White defendants were often predicted to be less risky than they were.</p></li>
<li><p>Black defendants were twice as likely as white defendants to be misclassified as being a higher
risk of violent recidivism.</p></li>
<li><p>Even when controlling for prior crimes, future recidivism, age, and gender, black defendants were
77 percent more likely to be assigned higher risk scores than white defendants.</p></li>
</ul>
</section>
<section id="data-description">
<h2>Data Description<a class="headerlink" href="#data-description" title="Permalink to this heading">#</a></h2>
<p>The authors of this article filed a public records request with the Broward County Sheriff’s office
in Florida.</p>
<p>Luckily for us, they did a significant amount of the legwork which is described in this
<a class="reference external" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">methodology article</a>.</p>
<p>We download the data below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_url</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/propublica/compas-analysis&quot;</span>
<span class="n">data_url</span> <span class="o">+=</span> <span class="s2">&quot;/master/compas-scores-two-years.csv&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_url</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>name</th>
      <th>first</th>
      <th>last</th>
      <th>compas_screening_date</th>
      <th>sex</th>
      <th>dob</th>
      <th>age</th>
      <th>age_cat</th>
      <th>race</th>
      <th>...</th>
      <th>v_decile_score</th>
      <th>v_score_text</th>
      <th>v_screening_date</th>
      <th>in_custody</th>
      <th>out_custody</th>
      <th>priors_count.1</th>
      <th>start</th>
      <th>end</th>
      <th>event</th>
      <th>two_year_recid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>miguel hernandez</td>
      <td>miguel</td>
      <td>hernandez</td>
      <td>2013-08-14</td>
      <td>Male</td>
      <td>1947-04-18</td>
      <td>69</td>
      <td>Greater than 45</td>
      <td>Other</td>
      <td>...</td>
      <td>1</td>
      <td>Low</td>
      <td>2013-08-14</td>
      <td>2014-07-07</td>
      <td>2014-07-14</td>
      <td>0</td>
      <td>0</td>
      <td>327</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>kevon dixon</td>
      <td>kevon</td>
      <td>dixon</td>
      <td>2013-01-27</td>
      <td>Male</td>
      <td>1982-01-22</td>
      <td>34</td>
      <td>25 - 45</td>
      <td>African-American</td>
      <td>...</td>
      <td>1</td>
      <td>Low</td>
      <td>2013-01-27</td>
      <td>2013-01-26</td>
      <td>2013-02-05</td>
      <td>0</td>
      <td>9</td>
      <td>159</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>ed philo</td>
      <td>ed</td>
      <td>philo</td>
      <td>2013-04-14</td>
      <td>Male</td>
      <td>1991-05-14</td>
      <td>24</td>
      <td>Less than 25</td>
      <td>African-American</td>
      <td>...</td>
      <td>3</td>
      <td>Low</td>
      <td>2013-04-14</td>
      <td>2013-06-16</td>
      <td>2013-06-16</td>
      <td>4</td>
      <td>0</td>
      <td>63</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5</td>
      <td>marcu brown</td>
      <td>marcu</td>
      <td>brown</td>
      <td>2013-01-13</td>
      <td>Male</td>
      <td>1993-01-21</td>
      <td>23</td>
      <td>Less than 25</td>
      <td>African-American</td>
      <td>...</td>
      <td>6</td>
      <td>Medium</td>
      <td>2013-01-13</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1</td>
      <td>0</td>
      <td>1174</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>bouthy pierrelouis</td>
      <td>bouthy</td>
      <td>pierrelouis</td>
      <td>2013-03-26</td>
      <td>Male</td>
      <td>1973-01-22</td>
      <td>43</td>
      <td>25 - 45</td>
      <td>Other</td>
      <td>...</td>
      <td>1</td>
      <td>Low</td>
      <td>2013-03-26</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2</td>
      <td>0</td>
      <td>1102</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 53 columns</p>
</div></div></div>
</div>
<p>We summarize some of the variables that we will use.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">first</span></code>: An individual’s first name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">last</span></code>: An individual’s last name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sex</span></code>: An individual’s sex</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">age</span></code>: An individual’s age</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">race</span></code>: An individual’s race. It takes values of Caucasian, Hispanic, African-American, Native American, Asian, or Other</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">priors_count</span></code>: Number of previous arrests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">decile_score</span></code>: The COMPAS risk score</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">two_year_recid</span></code>: Whether the individual had been jailed for a new crime in next two years</p></li>
</ul>
</section>
<section id="descriptive-statistics">
<h2>Descriptive Statistics<a class="headerlink" href="#descriptive-statistics" title="Permalink to this heading">#</a></h2>
<p>The first thing we do with our data is to drop any classes without “enough” observations.</p>
<p>One of our focuses will be on inter-race differences in scores and recidivism, so we only
keep data on races with at least 500 observations in our data set.</p>
<p>Just be aware that this kind of seemingly and even genuinely benign or “technical” decision can still perpetuate inequality by exclusion.</p>
<p>For example, Asians are a small minority, so they’re not really present in the data, and therefore they’re absent from the policy discussion — we have no inferential knowledge on how COMPAS scores work for them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">race_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;race&quot;</span><span class="p">])[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">at_least_500</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">race_count</span><span class="p">[</span><span class="n">race_count</span> <span class="o">&gt;</span> <span class="mi">500</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The following race have at least 500 observations:&quot;</span><span class="p">,</span> <span class="n">at_least_500</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">at_least_500</span><span class="p">),</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The following race have at least 500 observations: [&#39;African-American&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;]
</pre></div>
</div>
</div>
</div>
<p>Next, we explore the remaining data using plots and tables.</p>
<section id="age-sex-and-race">
<h3>Age, Sex, and Race<a class="headerlink" href="#age-sex-and-race" title="Permalink to this heading">#</a></h3>
<p>Let’s look at how the dataset is broken down into age, sex, and race.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_groupcount_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">group_col</span><span class="p">,</span> <span class="n">figsize</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s2">&quot;call df.groupby(group_col), then count number of records and plot&quot;</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">group_col</span><span class="p">)[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">counts</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;top&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">age_cs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Less than 25&quot;</span><span class="p">,</span> <span class="s2">&quot;25 - 45&quot;</span><span class="p">,</span> <span class="s2">&quot;Greater than 45&quot;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;age_cat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;age_cat&quot;</span><span class="p">],</span> <span class="n">categories</span><span class="o">=</span><span class="n">age_cs</span><span class="p">,</span> <span class="n">ordered</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">create_groupcount_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;age_cat&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;DarkBlue&quot;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d9474084b7f6cfe93fd313c6a88ebca3d1b0deb4493fb86e34d4fd84eaaa4a0a.png" src="../_images/d9474084b7f6cfe93fd313c6a88ebca3d1b0deb4493fb86e34d4fd84eaaa4a0a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sex_cs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;sex&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;sex&quot;</span><span class="p">],</span> <span class="n">categories</span><span class="o">=</span><span class="n">sex_cs</span><span class="p">,</span> <span class="n">ordered</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">create_groupcount_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;DarkBlue&quot;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 600x800 with 1 Axes&gt;, &lt;Axes: &gt;)
</pre></div>
</div>
<img alt="../_images/176504b6742d2f3d865c7249c196e943dd1c6c3f9d359ede81a3614bd75ff973.png" src="../_images/176504b6742d2f3d865c7249c196e943dd1c6c3f9d359ede81a3614bd75ff973.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">race_cs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;race&quot;</span><span class="p">],</span> <span class="n">categories</span><span class="o">=</span><span class="n">race_cs</span><span class="p">,</span> <span class="n">ordered</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">create_groupcount_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;DarkBlue&quot;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 1200x800 with 1 Axes&gt;, &lt;Axes: &gt;)
</pre></div>
</div>
<img alt="../_images/9cb49e9aca6e0606f5ff45837306f77c30fe4011ca97e723a97a4680bb2bd1fe.png" src="../_images/9cb49e9aca6e0606f5ff45837306f77c30fe4011ca97e723a97a4680bb2bd1fe.png" />
</div>
</div>
<p>From this, we learn that our population is mostly between 25-45, male, and
is mostly African-American or Caucasian.</p>
</section>
<section id="recidivism">
<h3>Recidivism<a class="headerlink" href="#recidivism" title="Permalink to this heading">#</a></h3>
<p>We now look into how recidivism is split across groups.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recid</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;age_cat&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">])[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="s2">&quot;race&quot;</span><span class="p">)</span>
<span class="n">recid</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>race</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
    </tr>
    <tr>
      <th>age_cat</th>
      <th>sex</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">Less than 25</th>
      <th>Female</th>
      <td>0.449704</td>
      <td>0.310345</td>
      <td>0.411765</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>0.645806</td>
      <td>0.541254</td>
      <td>0.536364</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">25 - 45</th>
      <th>Female</th>
      <td>0.382278</td>
      <td>0.423948</td>
      <td>0.333333</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>0.533074</td>
      <td>0.433699</td>
      <td>0.375000</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">Greater than 45</th>
      <th>Female</th>
      <td>0.227273</td>
      <td>0.239766</td>
      <td>0.217391</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>0.425101</td>
      <td>0.289157</td>
      <td>0.216667</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In the table, we see that the young have higher recidivism rates than the old, except for among
Caucasian females.</p>
<p>Also, African-American males are at a particularly high risk of recidivism even as they get older.</p>
</section>
<section id="risk-scores">
<h3>Risk Scores<a class="headerlink" href="#risk-scores" title="Permalink to this heading">#</a></h3>
<p>Each individual in the dataset was assigned a <code class="docutils literal notranslate"><span class="pre">decile_score</span></code> ranging from 1 to 10.</p>
<p>This score represents the perceived risk of recidivism with 1 being the lowest risk and 10 being the highest.</p>
<p>We show a bar plot of all decile scores below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">create_groupcount_barplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;decile_score&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;DarkBlue&quot;</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 1200x800 with 1 Axes&gt;, &lt;Axes: &gt;)
</pre></div>
</div>
<img alt="../_images/9eb53edd7eee65f78cb249eaaa0adbf5ad6621d790a6b94694cd4de9ec37dbcc.png" src="../_images/9eb53edd7eee65f78cb249eaaa0adbf5ad6621d790a6b94694cd4de9ec37dbcc.png" />
</div>
</div>
<p>How do these scores differ by race?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfgb</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;race&quot;</span><span class="p">)</span>
<span class="n">race_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;race&quot;</span><span class="p">)[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">race</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]):</span>
    <span class="p">(</span>
        <span class="p">(</span><span class="n">dfgb</span>
            <span class="o">.</span><span class="n">get_group</span><span class="p">(</span><span class="n">race</span><span class="p">)</span>
            <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile_score&quot;</span><span class="p">)[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">race_count</span><span class="p">[</span><span class="n">race</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#353535&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">race</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="c1"># set equal y limit for visual comparison</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Score Frequency by Race&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0.98, &#39;Score Frequency by Race&#39;)
</pre></div>
</div>
<img alt="../_images/3ca47d476759460ab442374b767ccbed2b2d162c6f6c63f0a3e7ba8783ba46b5.png" src="../_images/3ca47d476759460ab442374b767ccbed2b2d162c6f6c63f0a3e7ba8783ba46b5.png" />
</div>
</div>
<p>While Caucasians and Hispanics both see the majority of their score distribution on low values,
African-Americans are almost equally likely to receive any score.</p>
</section>
<section id="risk-scores-and-recidivism">
<h3>Risk Scores and Recidivism<a class="headerlink" href="#risk-scores-and-recidivism" title="Permalink to this heading">#</a></h3>
<p>Now we can explore the relationship between the risk score and actual two year recidivism.</p>
<p>The first measure we look at is the frequency of recidivism by decile score – these numbers
tell us what percentage of people assigned a particular risk score committed a new crime within two
years of being released.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;decile_score&quot;</span><span class="p">)[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>decile_score
1     0.220392
2     0.309112
3     0.375887
4     0.426593
5     0.478723
6     0.564228
7     0.590988
8     0.681363
9     0.698795
10    0.770889
Name: two_year_recid, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Let’s also look at the correlation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[[</span><span class="s2">&quot;decile_score&quot;</span><span class="p">,</span> <span class="s2">&quot;two_year_recid&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>decile_score</th>
      <th>two_year_recid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>decile_score</th>
      <td>1.000000</td>
      <td>0.346797</td>
    </tr>
    <tr>
      <th>two_year_recid</th>
      <td>0.346797</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As the risk score increases, the percentage of people committing a new crime does as well, with a
positive correlation (~0.35).</p>
<p>This is good news – it means that the score is producing at least some signal about an individual’s recidivism risk.</p>
<p>One of the key critiques from Pro Publica, though, was that the inaccuracies were nonuniform — that is, the tool was systematically wrong about certain populations.</p>
<p>Let’s now separate the correlations by race and see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recid_rates</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">&quot;decile_score&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">)</span>

<span class="n">recid_rates</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>race</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
    </tr>
    <tr>
      <th>decile_score</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.228643</td>
      <td>0.208517</td>
      <td>0.244898</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.302799</td>
      <td>0.313019</td>
      <td>0.318584</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.419075</td>
      <td>0.340659</td>
      <td>0.313953</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.459740</td>
      <td>0.396491</td>
      <td>0.346154</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.482192</td>
      <td>0.460581</td>
      <td>0.538462</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.559896</td>
      <td>0.572165</td>
      <td>0.567568</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.592500</td>
      <td>0.615385</td>
      <td>0.470588</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.682451</td>
      <td>0.719298</td>
      <td>0.500000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.707895</td>
      <td>0.693878</td>
      <td>0.550000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.793706</td>
      <td>0.703125</td>
      <td>0.666667</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Or, in plotted form,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">_race</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]):</span>
    <span class="n">_rr_vals</span> <span class="o">=</span> <span class="n">recid_rates</span><span class="p">[</span><span class="n">_race</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">_rr_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#c60000&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">_rr_vals</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="n">_rr_vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#353535&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">_race</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;top&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;bottom&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">tick_right</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Recidivism Rates by Race&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0.98, &#39;Recidivism Rates by Race&#39;)
</pre></div>
</div>
<img alt="../_images/167109a4a47993d5eb0b7dc2650193cdef8538e23b683a5d408bbc0a9b8c1650.png" src="../_images/167109a4a47993d5eb0b7dc2650193cdef8538e23b683a5d408bbc0a9b8c1650.png" />
</div>
</div>
</section>
</section>
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this heading">#</a></h2>
<p>In what follows, we will be doing something slightly different than what was done in the Pro Publica
article.</p>
<p>First, we will explore what happens when we try to predict the COMPAS risk scores using the
observable data that we have.</p>
<p>Second, we will use binary probability models to predict whether an individual is at risk of
recidivism.</p>
<p>We will do this first using the COMPAS risk scores, and then afterwards we will try to write our own
model based on raw observables, like age, race and sex.</p>
<section id="preprocessing">
<h3>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this heading">#</a></h3>
<p>We would like to use some features that are inherently non-numerical such as sex, age group, and
race in our model.</p>
<p>Before we can do that, we need to encode these string values as numerical values
so our machine learning algorithms can understand them – an econometrician would call this,
creating dummy variables.</p>
<p><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> can automatically do this for us using <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code>.</p>
<p>Essentially, we make one column for each possible value of a categorical
variable and then we set just one of these columns equal to a 1 if the observation has that
column’s category, and set all other columns to 0.</p>
<p>Let’s do an example.</p>
<p>Imagine we have the array below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="s2">&quot;Male&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Male&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Male&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>The way to encode this would be to create the array below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sex_encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> it would be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ohe</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sex_ohe</span> <span class="o">=</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sex</span><span class="p">)</span>

<span class="c1"># This should shows 0s!</span>
<span class="n">sex_ohe</span> <span class="o">-</span> <span class="n">sex_encoded</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.],
       [0., 0.]])
</pre></div>
</div>
</div>
</div>
<p>We will use this encoding trick below as we create our data.</p>
</section>
<section id="predicting-compas-scores">
<h3>Predicting COMPAS Scores<a class="headerlink" href="#predicting-compas-scores" title="Permalink to this heading">#</a></h3>
<p>First, we proceed by creating the <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> inputs into a manageable format.</p>
<p>We encode the categorical variables using the <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code> described above, and then merge that with the non-categorical data.</p>
<p>Finally, we split the data into training and validation (test) subsets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prep_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">continuous_variables</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">):</span>

    <span class="n">ohe</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_var</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># Add continuous variables if exist</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">continuous_variables</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">continuous_variables</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">categories</span><span class="p">])])</span>

    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span>
</pre></div>
</div>
</div>
</div>
<p>As we proceed, our goal will be to see which variables are most important for predicting the COMPAS
scores.</p>
<p>As we estimate these models, one of our metrics for success will be mean absolute error (MAE).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_and_report_maes</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_inv_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">yhat_train</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">yhat_test</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">yhat_train</span> <span class="o">=</span> <span class="n">y_inv_transform</span><span class="p">(</span><span class="n">yhat_train</span><span class="p">)</span>
        <span class="n">yhat_test</span> <span class="o">=</span> <span class="n">y_inv_transform</span><span class="p">(</span><span class="n">yhat_test</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mae_train</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">yhat_train</span><span class="p">),</span>
        <span class="n">mae_test</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat_test</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s begin with a simple linear model which uses just prior arrests.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">prep_data</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;priors_count&quot;</span><span class="p">],</span> <span class="p">[],</span> <span class="s2">&quot;decile_score&quot;</span>
<span class="p">)</span>

<span class="n">fit_and_report_maes</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mae_train&#39;: 2.162527833108664, &#39;mae_test&#39;: 2.191754484529134}
</pre></div>
</div>
</div>
</div>
<p>This simple model obtains a MAE of about 2 for both the test data and training data.</p>
<p>This means, on average, that our model can predict the COMPAS score (which ranges from 1-10)
within about 2 points.</p>
<p>While the MAE is about 2, knowing what the errors on our prediction model look
like is often very useful.</p>
<p>Below, we create a histogram which shows the distribution of these errors. In our case, we
take the difference between predicted value and actual value, so a positive value means that we
overpredicted the COMPAS score and a negative value means we underpredicted it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">yhat_train</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">yhat_test</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">yhat_train</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">yhat_test</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Test Data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Test Data&#39;)
</pre></div>
</div>
<img alt="../_images/f564471a75aa4ba052f44e6d92f51e412e0f058b42cacd2e1d0e8fd711ab1544.png" src="../_images/f564471a75aa4ba052f44e6d92f51e412e0f058b42cacd2e1d0e8fd711ab1544.png" />
</div>
</div>
<p>In both cases, the long left tails of errors suggest the existence of relevant features which would improve our model.</p>
<p>The first thing we might consider investigating is whether there are non-linearities in how the
number of priors enters the COMPAS score.</p>
<p>First, we try using polynomial features in our exogenous variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">prep_data</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;priors_count&quot;</span><span class="p">],</span> <span class="p">[],</span> <span class="s2">&quot;decile_score&quot;</span>
<span class="p">)</span>

<span class="c1"># Transform data to quadratic</span>
<span class="n">pf</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">fit_and_report_maes</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mae_train&#39;: 2.120405801755292, &#39;mae_test&#39;: 2.1179838134597335}
</pre></div>
</div>
</div>
</div>
<p>We don’t see a very significant increase in performance, so we also try using log on the endogenous
variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">prep_data</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;priors_count&quot;</span><span class="p">],</span> <span class="p">[],</span> <span class="s2">&quot;decile_score&quot;</span>
<span class="p">)</span>

<span class="n">fit_and_report_maes</span><span class="p">(</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>
    <span class="n">y_transform</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">,</span> <span class="n">y_inv_transform</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mae_train&#39;: 2.2550821558610115, &#39;mae_test&#39;: 2.3332184125647917}
</pre></div>
</div>
</div>
</div>
<p>Still no improvement… The next natural thing is to add more features to our regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">prep_data</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;priors_count&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;age_cat&quot;</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">],</span> <span class="s2">&quot;decile_score&quot;</span>
<span class="p">)</span>

<span class="n">fit_and_report_maes</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;mae_train&#39;: 1.8076563650603423, &#39;mae_test&#39;: 1.8277010173497896}
</pre></div>
</div>
</div>
</div>
<p>By allowing for indicator variables on age, race, and sex, we are able to slightly improve the MAE.
The errors also seem to have a less extreme tail.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">prep_data</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;priors_count&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;age_cat&quot;</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">],</span> <span class="s2">&quot;decile_score&quot;</span>
<span class="p">)</span>

<span class="n">lr_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">yhat_train</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">yhat_test</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">yhat_train</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">yhat_test</span> <span class="o">-</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Test Data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/share/miniconda3/envs/lecture-datascience/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Test Data&#39;)
</pre></div>
</div>
<img alt="../_images/ceafb02590242c52528d4d83d57a3cc2f2271ea4412cba0a4aeb0919f68fdd5a.png" src="../_images/ceafb02590242c52528d4d83d57a3cc2f2271ea4412cba0a4aeb0919f68fdd5a.png" />
</div>
</div>
<p>The coefficients are listed below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;priors_count&quot;</span><span class="p">,</span> <span class="s2">&quot;Less than 25&quot;</span><span class="p">,</span> <span class="s2">&quot;25-45&quot;</span><span class="p">,</span> <span class="s2">&quot;Greater than 45&quot;</span><span class="p">,</span> <span class="s2">&quot;African-American&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">,</span> <span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span>
<span class="p">]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="n">_coef</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">_name</span><span class="p">,</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">_coef</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>priors_count :  0.2799923049684579
Less than 25 :  -0.10262755807946589
25-45 :  -1.7189018948181365
Greater than 45 :  1.8215294528975996
African-American :  0.6102522379830785
Caucasian :  -0.10156939689442401
Hispanic :  -0.5086828410886544
Female :  0.04293582427466183
Male :  -0.04293582427466186
</pre></div>
</div>
</div>
</div>
<p>What stands out to you about these coefficients?</p>
<div class="admonition-exercise admonition" id="app-rcd-dir1">
<p class="admonition-title">Exercise</p>
<p>See exercise 1 in the <a class="reference internal" href="#app-rcd-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</section>
<section id="binary-probability-models">
<h3>Binary Probability Models<a class="headerlink" href="#binary-probability-models" title="Permalink to this heading">#</a></h3>
<p>Binary probability models are used to model “all or nothing” outcomes,
like the occurrence of an event.</p>
<p>Their output is the probability that an event of interest occurs.</p>
<p>With this probability in hand, the researcher chooses an acceptable cutoff (perhaps 0.5)
above which the event is predicted to occur.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Binary probability models can be thought of as a special case of
classification.</p>
<p>In classification, we are given a set of features and asked to predict
one of a finite number of discrete labels.</p>
<p>We will learn more about classification in an upcoming lecture!</p>
</div>
<p>In our example, we will be interested in how the COMPAS scores do at predicting recidivism and how
their ability to predict depends on race or sex.</p>
<p>To assist us in evaluating the performance of various models we will use a new
metric called the <em>confusion matrix</em>.</p>
<p>Scikit-learn knows how to compute this metric and also provides a good description
of what is computed.</p>
<p>Let’s see what they have to say.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Help on function confusion_matrix in module sklearn.metrics._classification:

confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)
    Compute confusion matrix to evaluate the accuracy of a classification.
    
    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`
    is equal to the number of observations known to be in group :math:`i` and
    predicted to be in group :math:`j`.
    
    Thus in binary classification, the count of true negatives is
    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is
    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.
    
    Read more in the :ref:`User Guide &lt;confusion_matrix&gt;`.
    
    Parameters
    ----------
    y_true : array-like of shape (n_samples,)
        Ground truth (correct) target values.
    
    y_pred : array-like of shape (n_samples,)
        Estimated targets as returned by a classifier.
    
    labels : array-like of shape (n_classes), default=None
        List of labels to index the matrix. This may be used to reorder
        or select a subset of labels.
        If ``None`` is given, those that appear at least once
        in ``y_true`` or ``y_pred`` are used in sorted order.
    
    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.
    
        .. versionadded:: 0.18
    
    normalize : {&#39;true&#39;, &#39;pred&#39;, &#39;all&#39;}, default=None
        Normalizes confusion matrix over the true (rows), predicted (columns)
        conditions or all the population. If None, confusion matrix will not be
        normalized.
    
    Returns
    -------
    C : ndarray of shape (n_classes, n_classes)
        Confusion matrix whose i-th row and j-th
        column entry indicates the number of
        samples with true label being i-th class
        and predicted label being j-th class.
    
    See Also
    --------
    ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix
        given an estimator, the data, and the label.
    ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix
        given the true and predicted labels.
    ConfusionMatrixDisplay : Confusion Matrix visualization.
    
    References
    ----------
    .. [1] `Wikipedia entry for the Confusion matrix
           &lt;https://en.wikipedia.org/wiki/Confusion_matrix&gt;`_
           (Wikipedia and other references may use a different
           convention for axes).
    
    Examples
    --------
    &gt;&gt;&gt; from sklearn.metrics import confusion_matrix
    &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
    &gt;&gt;&gt; confusion_matrix(y_true, y_pred)
    array([[2, 0, 0],
           [0, 0, 1],
           [1, 0, 2]])
    
    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]
    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]
    &gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])
    array([[2, 0, 0],
           [0, 0, 1],
           [1, 0, 2]])
    
    In the binary case, we can extract true positives, etc as follows:
    
    &gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
    &gt;&gt;&gt; (tn, fp, fn, tp)
    (0, 2, 1, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">report_cm</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
     <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
         <span class="n">cm_train</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)),</span>
         <span class="n">cm_test</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
     <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will start by using logistic regression using only <code class="docutils literal notranslate"><span class="pre">decile_score</span></code>
as a feature and then examine how the confusion matrices differ by
race and sex.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>
<span class="n">groups</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;overall&quot;</span><span class="p">,</span> <span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">,</span> <span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span>
<span class="p">]</span>

<span class="n">ind</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Portion_of_NoRecid_and_LowRisk&quot;</span><span class="p">,</span> <span class="s2">&quot;Portion_of_Recid_and_LowRisk&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Portion_of_NoRecid_and_HighRisk&quot;</span><span class="p">,</span> <span class="s2">&quot;Portion_of_Recid_and_HighRisk&quot;</span>
<span class="p">]</span>

<span class="n">fmla</span> <span class="o">=</span> <span class="s2">&quot;two_year_recid ~ C(decile_score)&quot;</span>
<span class="n">y</span><span class="p">,</span><span class="n">X</span> <span class="o">=</span> <span class="n">dmatrices</span><span class="p">(</span><span class="n">fmla</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>


<span class="n">decile_mod</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;lbfgs&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cm_tables</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">ind</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">subset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">y_sub</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">pred_sub</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>

        <span class="n">cm</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_sub</span><span class="p">,</span> <span class="n">pred_sub</span><span class="p">)</span>

        <span class="c1"># Compute fraction for which the guess is correct</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cm</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">cond_probs</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">d</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span>
        <span class="n">pcm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">col</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">d</span><span class="p">)</span>
        <span class="n">pcm</span> <span class="o">=</span> <span class="n">pcm</span><span class="o">/</span><span class="n">pcm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="n">pcm</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">given_outcome</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">given_outcome</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;P(LowRisk|NoRecid)&quot;</span><span class="p">,</span><span class="s2">&quot;P(HighRisk|NoRecid)&quot;</span><span class="p">,</span><span class="s2">&quot;P(LowRisk|Recid)&quot;</span><span class="p">,</span><span class="s2">&quot;P(HighRisk|Recid)&quot;</span><span class="p">]</span>
    <span class="n">given_outcome</span><span class="o">=</span><span class="n">given_outcome</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">cond_probs</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">given_pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">given_pred</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;P(NoRecid|LowRisk)&quot;</span><span class="p">,</span><span class="s2">&quot;P(NoRecid|HighRisk)&quot;</span><span class="p">,</span><span class="s2">&quot;P(Recid|LowRisk)&quot;</span><span class="p">,</span><span class="s2">&quot;P(Recid|HighRisk)&quot;</span><span class="p">]</span>
    <span class="n">given_pred</span><span class="o">=</span><span class="n">given_pred</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">cond_probs</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">given_outcome</span><span class="p">,</span> <span class="n">given_pred</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">given_outcome</span><span class="p">,</span> <span class="n">given_pred</span> <span class="o">=</span><span class="n">cm_tables</span><span class="p">(</span><span class="n">decile_mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
                                             <span class="n">y_test</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Portion_of_NoRecid_and_LowRisk</th>
      <td>0.361815</td>
      <td>0.263270</td>
      <td>0.475000</td>
      <td>0.522581</td>
      <td>0.438040</td>
      <td>0.342222</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_LowRisk</th>
      <td>0.191514</td>
      <td>0.230361</td>
      <td>0.136667</td>
      <td>0.167742</td>
      <td>0.207493</td>
      <td>0.187407</td>
    </tr>
    <tr>
      <th>Portion_of_NoRecid_and_HighRisk</th>
      <td>0.152033</td>
      <td>0.140127</td>
      <td>0.168333</td>
      <td>0.161290</td>
      <td>0.141210</td>
      <td>0.154815</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_HighRisk</th>
      <td>0.294638</td>
      <td>0.366242</td>
      <td>0.220000</td>
      <td>0.148387</td>
      <td>0.213256</td>
      <td>0.315556</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">output</span></code> contains information on the percent of true negatives, false negatives, false positives,
and true positives.</p>
<p>What do you see?</p>
<p>The joint probabilities (of prediction and outcome given race or sex) in the
above table are a bit hard to interpret.</p>
<p>Conditional probabilities can be easier to think about.</p>
<p>Let’s look at the probability of outcomes given the prediction as well as race or sex.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">given_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(NoRecid|LowRisk)</th>
      <td>0.704128</td>
      <td>0.652632</td>
      <td>0.738342</td>
      <td>0.764151</td>
      <td>0.756219</td>
      <td>0.688525</td>
    </tr>
    <tr>
      <th>P(NoRecid|HighRisk)</th>
      <td>0.393939</td>
      <td>0.386121</td>
      <td>0.383178</td>
      <td>0.530612</td>
      <td>0.493151</td>
      <td>0.372607</td>
    </tr>
    <tr>
      <th>P(Recid|LowRisk)</th>
      <td>0.295872</td>
      <td>0.347368</td>
      <td>0.261658</td>
      <td>0.235849</td>
      <td>0.243781</td>
      <td>0.311475</td>
    </tr>
    <tr>
      <th>P(Recid|HighRisk)</th>
      <td>0.606061</td>
      <td>0.613879</td>
      <td>0.616822</td>
      <td>0.469388</td>
      <td>0.506849</td>
      <td>0.627393</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As you can see, the distribution of outcomes conditional on predictions does
not vary too much with race.</p>
<p>Moreover, if anything, it discriminates in favor of African-Americans.</p>
<p>The algorithm does appear to overpredict recidivism for women compared
to men.</p>
<p>This is an important concern.</p>
<p>We will not discuss it too much though because (1) we will see below that
when fairness is looked at in another way, women are favored over men,
and (2) the company that produces COMPAS also produces a separate
questionnaire and risk score designed only for women.</p>
</section>
<section id="false-positive-and-negative-rates">
<h3>False Positive and Negative Rates<a class="headerlink" href="#false-positive-and-negative-rates" title="Permalink to this heading">#</a></h3>
<p>What if we flip this around and look at the distributions of
predictions conditional on outcomes?</p>
<p>Why look at these probabilities?</p>
<p>One reason is that in law, it’s traditionally far
worse to punish innocents than let the guilty free. This idea goes at
least back to <a class="reference external" href="https://en.wikipedia.org/wiki/Blackstone%27s_ratio">1760 and Blackstone’s ratio</a>.</p>
<blockquote>
<div><p>It is better that ten guilty persons escape than that one innocent
suffer. -William Blackstone</p>
</div></blockquote>
<p>Blackstone’s ratio says that we should be particularly concerned about
P(HighRisk | NoRecid).</p>
<p>This probability is also called the false
positive rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">given_outcome</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(LowRisk|NoRecid)</th>
      <td>0.653887</td>
      <td>0.533333</td>
      <td>0.776567</td>
      <td>0.757009</td>
      <td>0.678571</td>
      <td>0.646154</td>
    </tr>
    <tr>
      <th>P(HighRisk|NoRecid)</th>
      <td>0.346113</td>
      <td>0.466667</td>
      <td>0.223433</td>
      <td>0.242991</td>
      <td>0.321429</td>
      <td>0.353846</td>
    </tr>
    <tr>
      <th>P(LowRisk|Recid)</th>
      <td>0.340369</td>
      <td>0.276730</td>
      <td>0.433476</td>
      <td>0.520833</td>
      <td>0.398374</td>
      <td>0.329134</td>
    </tr>
    <tr>
      <th>P(HighRisk|Recid)</th>
      <td>0.659631</td>
      <td>0.723270</td>
      <td>0.566524</td>
      <td>0.479167</td>
      <td>0.601626</td>
      <td>0.670866</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we see some large disparities by race in the false positive rate
(and false negative rate). This is one of the main findings of the Pro
Publica article.</p>
<p>In response to Pro Publica, Northpointe, the company that produces
COMPAS, argued that COMPAS is not biased because the probabilities of
outcomes conditional on predictions (like P(NoRecid|LowRisk)) are
approximately equal across races <span id="id1">[<a class="reference internal" href="#id16" title="William Dieterich, Christina Mendoza, and Tim Brennan. Compas risk scales: demonstrating accuracy equity and predictive parity. Northpoint Inc, 2016.">recidDMB16</a>]</span>.</p>
<p>Following <span id="id2">[<a class="reference internal" href="#id6" title="Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human Decisions and Machine Predictions*. The Quarterly Journal of Economics, 133(1):237-293, 08 2017. URL: https://dx.doi.org/10.1093/qje/qjx032, arXiv:http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf, doi:10.1093/qje/qjx032.">recidKLL+17</a>]</span>, we will call a prediction algorithm with this
property  well-calibrated.</p>
<p>Being well-calibrated is one criteria for fairness of a prediction algorithm.</p>
<p>Pro Publica’s critique focuses on a different criteria – that the the
probability of predicted categories conditional on true outcomes
should be equal across groups (i.e. P(HighRisk|NoRecid) should be
equal across races).</p>
<p><span id="id3">[<a class="reference internal" href="#id6" title="Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human Decisions and Machine Predictions*. The Quarterly Journal of Economics, 133(1):237-293, 08 2017. URL: https://dx.doi.org/10.1093/qje/qjx032, arXiv:http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf, doi:10.1093/qje/qjx032.">recidKLL+17</a>]</span> calls a prediction algorithm with this property balanced.</p>
</section>
<section id="visualizing-calibration-and-balance">
<h3>Visualizing Calibration and Balance<a class="headerlink" href="#visualizing-calibration-and-balance" title="Permalink to this heading">#</a></h3>
<p>We can get a slightly more detailed look at calibration and balance by
recognizing that prediction algorithms typically compute a predicted
probability, not just a discrete predicted outcome.</p>
<p>The predicted outcome will typically be assigned to the category with the highest
predicted probability.</p>
<p>We can examine calibration graphically by plotting the P(recidivism | predicted probability)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy</span>

<span class="k">def</span> <span class="nf">calibration_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">g</span><span class="p">,</span><span class="n">group</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">subset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">)]</span>
        <span class="n">y_sub</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">pred_sub</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binned_statistic</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">,</span><span class="n">y_sub</span><span class="p">,</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
        <span class="n">se</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binned_statistic</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">,</span><span class="n">y_sub</span><span class="p">,</span>
                         <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
        <span class="n">midpts</span> <span class="o">=</span> <span class="p">(</span><span class="n">edges</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">edges</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">midpts</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="mf">1.64</span><span class="o">*</span><span class="n">se</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Observed recidivism&quot;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted P(recidivism)&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">_ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">())</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>

<span class="n">calibration_plot</span><span class="p">(</span><span class="n">decile_mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span>
                 <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">],</span>
                 <span class="n">df_test</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b615d0eba7a26133a1d93538bbd00becf08cd01a46968f39e3f5f814f7182826.png" src="../_images/b615d0eba7a26133a1d93538bbd00becf08cd01a46968f39e3f5f814f7182826.png" />
</div>
</div>
<p>This figure is one way to visualize how well-calibrated these
predictions are.</p>
<p>The dots are binned averages of observed recidivism, conditional on
predicted recidivism being in some range.</p>
<p>The error bars represent a 90% confidence interval.</p>
<p>A perfectly calibrated prediction would have these dots all lie
along the 45 degree line.</p>
<p>For dots below the 45 degree line, the algorithm is overpredicting
recidivism.</p>
<div class="admonition-exercise admonition" id="app-rcd-dir2">
<p class="admonition-title">Exercise</p>
<p>See exercise 2 in the <a class="reference internal" href="#app-rcd-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
<p>The algorithm appears fairly well-calibrated.</p>
<p>It does not seem to be making systematic errors in one direction based on any particular
race– but it does appear to be systematic overprediction for females compared to males.</p>
<p>Now, let’s create a figure to examine balance.</p>
<p>Balance is about the distribution of predictions conditional on outcomes,
so we will plot histograms of predicted probabilities conditional on realized
outcomes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="k">def</span> <span class="nf">balance_hist_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">g</span><span class="p">,</span><span class="n">group</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">subset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">)]</span>
        <span class="n">y_sub</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">pred_sub</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">[</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">_ax</span><span class="p">,</span>
                     <span class="n">label</span><span class="o">=</span><span class="s2">&quot;No recidivate&quot;</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axlabel</span><span class="o">=</span><span class="s2">&quot;Predicted Probability&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">[</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">_ax</span><span class="p">,</span>
                     <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Yes recidivate&quot;</span><span class="p">,</span> <span class="n">norm_hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axlabel</span><span class="o">=</span><span class="s2">&quot;Predicted Probability&quot;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>

<span class="n">balance_hist_plot</span><span class="p">(</span><span class="n">decile_mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">],</span>
                  <span class="n">df_test</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2722/1257276985.py:14: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(pred_sub[y_sub==0], hist=True, bins=bins, kde=False, ax=_ax,
/tmp/ipykernel_2722/1257276985.py:16: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(pred_sub[y_sub==1], hist=True, bins=bins, kde=False, ax=_ax,
</pre></div>
</div>
<img alt="../_images/175a74e6c61a8674ad08da8818f53f3d12d4b9d2799861d35550f4167933c6e0.png" src="../_images/175a74e6c61a8674ad08da8818f53f3d12d4b9d2799861d35550f4167933c6e0.png" />
</div>
</div>
<p>This figure is somewhat useful, but not for depicting balance
especially clearly, so let’s try something else.</p>
<p>To get false positive and false negative rates, we must assign the predicted
probabilities to outcomes.</p>
<p>The most common choice would be to predict recidivism if the predicted
probability is greater than 0.5.</p>
<p>However, if we want to adjust the false positive and false negative rates, we
might want to choose some other threshold and predict recidivism if
the predicted probability exceeds this threshold.</p>
<p>Different thresholds will lead to different false negative and false
positive rates, so let’s plot these rates as functions of the threshold.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">balance_threshold_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">bins</span><span class="p">)</span>
    <span class="c1"># get colors defined by theme</span>
    <span class="n">colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;African-American&quot;</span><span class="p">,</span> <span class="s2">&quot;Caucasian&quot;</span><span class="p">,</span> <span class="s2">&quot;Hispanic&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">group</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">]:</span>
            <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="n">group</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">y_sub</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">pred_sub</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span>
        <span class="n">_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">[</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;</span><span class="n">xi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">c1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fn</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">fn</span><span class="p">)</span><span class="o">/</span><span class="n">c1</span><span class="p">)</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_sub</span><span class="p">[</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;</span><span class="n">xi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
        <span class="n">c0</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_sub</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fp</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">fp</span><span class="p">)</span><span class="o">/</span><span class="n">c0</span><span class="p">)</span>
        <span class="n">p</span><span class="o">=</span><span class="n">_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fn</span><span class="o">-</span><span class="mf">1.64</span><span class="o">*</span><span class="n">sen</span><span class="p">,</span> <span class="n">fn</span><span class="o">+</span><span class="mf">1.64</span><span class="o">*</span><span class="n">sen</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">bins</span><span class="o">//</span><span class="mi">7</span><span class="o">*</span><span class="n">g</span><span class="p">],</span> <span class="n">fn</span><span class="p">[</span><span class="n">bins</span><span class="o">//</span><span class="mi">7</span><span class="o">*</span><span class="n">g</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;False +/- Rate&quot;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Threshold&quot;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;False Negative Rate&quot;</span><span class="p">)</span>

        <span class="n">_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">r</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">p</span><span class="o">=</span><span class="n">_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fp</span><span class="o">-</span><span class="mf">1.64</span><span class="o">*</span><span class="n">sep</span><span class="p">,</span> <span class="n">fp</span><span class="o">+</span><span class="mf">1.64</span><span class="o">*</span><span class="n">sep</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Threshold&quot;</span><span class="p">)</span>
        <span class="n">_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>

<span class="n">balance_threshold_plot</span><span class="p">(</span><span class="n">decile_mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span>
                       <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;two_year_recid&quot;</span><span class="p">],</span>
                       <span class="n">df_test</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/69dc7b98976cb3bb25459b111986fbda3ba2bd9e9c53171b885df493681f1fe4.png" src="../_images/69dc7b98976cb3bb25459b111986fbda3ba2bd9e9c53171b885df493681f1fe4.png" />
</div>
</div>
<p>From this, we can more easily see the balance problem — regardless
of which threshold we choose, African-Americans will have a higher
false positive rate than Caucasians.</p>
<p>We have seen that COMPAS scores are well-calibrated conditional on
race, but not balanced.</p>
<p>Can we create an alternative prediction that is both well-calibrated and balanced?</p>
</section>
<section id="creating-an-alternative-prediction">
<h3>Creating an Alternative Prediction<a class="headerlink" href="#creating-an-alternative-prediction" title="Permalink to this heading">#</a></h3>
<p>As a starting exercise, let’s predict recidivism using the variables
in this dataset other than race and COMPAS score.</p>
<p>Almost all variables in this data are categorical.</p>
<p>Any function of categorical variables can be represented as a linear
function of indicator variables and their interactions.</p>
<p>Given that linearity in indicators does not impose any substantiative restriction
here, a penalized linear model like lasso seems like a good choice for prediction.</p>
<p>To keep the computation time reasonable, we do not include all interaction
and indicator terms here.</p>
<p>To ensure that predicted probabilities are between 0 and 1, we fit a logistic
regression with an <span class="math notranslate nohighlight">\(\ell-1\)</span> penalty.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>

<span class="c1"># charge_desc has many values with one observations, we will</span>
<span class="c1"># combine these descriptions into a single &quot;other.&quot; This could</span>
<span class="c1"># be improved upon by looking at the text of descriptions and</span>
<span class="c1"># combining.</span>
<span class="n">df</span><span class="o">.</span><span class="n">c_charge_desc</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">c_charge_desc</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;charge_cat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">c_charge_desc</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">c_charge_desc</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="n">df</span><span class="o">.</span><span class="n">c_charge_desc</span><span class="p">]</span>
<span class="n">cnt</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">cnt</span><span class="o">&lt;</span><span class="mi">10</span><span class="p">,</span><span class="s2">&quot;charge_cat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;other&quot;</span>
<span class="n">df</span><span class="o">.</span><span class="n">charge_cat</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">charge_cat</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">sex</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sex</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>


<span class="n">fmla</span> <span class="o">=</span> <span class="s2">&quot;two_year_recid ~ sex*(age + juv_fel_count + juv_misd_count + juv_other_count + C(priors_count) + c_charge_degree + charge_cat)&quot;</span>

<span class="n">y</span><span class="p">,</span><span class="n">X</span> <span class="o">=</span> <span class="n">dmatrices</span><span class="p">(</span><span class="n">fmla</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There are </span><span class="si">{}</span><span class="s2"> features&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">),</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">lasso_mod</span><span class="o">=</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span>
                                            <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                            <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_log_loss&quot;</span><span class="p">,</span>
                                            <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>There are 260 features
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the regularization parameter chosen and the non-zero coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plots illustrating regularization parameter choice</span>
<span class="n">scores</span><span class="o">=</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">logpenalties</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span>
<span class="n">nnonzero</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">&gt;</span><span class="mf">1e-6</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logpenalties</span><span class="p">,</span><span class="n">scores</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;CV log likelihood&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;-log(penalty)&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logpenalties</span><span class="p">,</span><span class="n">nnonzero</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;nonzero coefficients&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e78877c5a4bdf7be5c0bca975134e3026a9d09b239aca988c95f3b15b94728bd.png" src="../_images/e78877c5a4bdf7be5c0bca975134e3026a9d09b239aca988c95f3b15b94728bd.png" />
</div>
</div>
<p>Let’s also look at the nonzero coefficients. We should be careful
about interpreting these, since relatively strong assumptions are
needed for lasso to produce consistent coefficient estimates.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lasso gives accurate predictions under weaker assumptions than needed for
consistent coefficient estimates.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># table of nonzero coefficients</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">design_info</span><span class="o">.</span><span class="n">column_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Value&quot;</span><span class="p">])</span>
<span class="n">coef</span><span class="o">.</span><span class="n">Value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">Value</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">1.0e-8</span><span class="p">))</span>
<span class="k">with</span> <span class="n">pd</span><span class="o">.</span><span class="n">option_context</span><span class="p">(</span><span class="s1">&#39;display.max_rows&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">Value</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">1.0e-8</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>82
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>0.008606</td>
    </tr>
    <tr>
      <th>sex[T.Male]</th>
      <td>0.724268</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.2]</th>
      <td>0.325185</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.3]</th>
      <td>0.612231</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.4]</th>
      <td>0.749700</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.5]</th>
      <td>0.732783</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.6]</th>
      <td>0.792306</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.7]</th>
      <td>1.032688</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.8]</th>
      <td>1.322570</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.9]</th>
      <td>1.286938</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.10]</th>
      <td>1.318479</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.11]</th>
      <td>1.236910</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.12]</th>
      <td>0.985708</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.13]</th>
      <td>1.528305</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.14]</th>
      <td>0.406061</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.15]</th>
      <td>0.781426</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.16]</th>
      <td>1.211658</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.17]</th>
      <td>0.371072</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.18]</th>
      <td>1.524265</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.19]</th>
      <td>1.143305</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.20]</th>
      <td>0.116819</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.21]</th>
      <td>1.298539</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.22]</th>
      <td>1.676822</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.23]</th>
      <td>1.332188</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.25]</th>
      <td>0.481937</td>
    </tr>
    <tr>
      <th>C(priors_count)[T.26]</th>
      <td>0.686574</td>
    </tr>
    <tr>
      <th>c_charge_degree[T.M]</th>
      <td>-0.228190</td>
    </tr>
    <tr>
      <th>charge_cat[T.Aggrav Battery w/Deadly Weapon]</th>
      <td>-0.152882</td>
    </tr>
    <tr>
      <th>charge_cat[T.Aggravated Assault W/dead Weap]</th>
      <td>-0.457757</td>
    </tr>
    <tr>
      <th>charge_cat[T.Aggravated Battery]</th>
      <td>-0.426229</td>
    </tr>
    <tr>
      <th>charge_cat[T.Battery]</th>
      <td>-0.008878</td>
    </tr>
    <tr>
      <th>charge_cat[T.Battery on Law Enforc Officer]</th>
      <td>0.312828</td>
    </tr>
    <tr>
      <th>charge_cat[T.Burglary Unoccupied Dwelling]</th>
      <td>-0.151937</td>
    </tr>
    <tr>
      <th>charge_cat[T.Criminal Mischief]</th>
      <td>0.441657</td>
    </tr>
    <tr>
      <th>charge_cat[T.DUI Level 0.15 Or Minor In Veh]</th>
      <td>-0.043787</td>
    </tr>
    <tr>
      <th>charge_cat[T.DUI Property Damage/Injury]</th>
      <td>-0.957453</td>
    </tr>
    <tr>
      <th>charge_cat[T.Driving License Suspended]</th>
      <td>-0.062541</td>
    </tr>
    <tr>
      <th>charge_cat[T.Driving Under The Influence]</th>
      <td>-0.141178</td>
    </tr>
    <tr>
      <th>charge_cat[T.Felony Driving While Lic Suspd]</th>
      <td>0.182792</td>
    </tr>
    <tr>
      <th>charge_cat[T.Felony Petit Theft]</th>
      <td>0.697398</td>
    </tr>
    <tr>
      <th>charge_cat[T.Grand Theft (Motor Vehicle)]</th>
      <td>0.134830</td>
    </tr>
    <tr>
      <th>charge_cat[T.Operating W/O Valid License]</th>
      <td>-0.084313</td>
    </tr>
    <tr>
      <th>charge_cat[T.Petit Theft]</th>
      <td>0.574935</td>
    </tr>
    <tr>
      <th>charge_cat[T.Petit Theft $100- $300]</th>
      <td>0.019328</td>
    </tr>
    <tr>
      <th>charge_cat[T.Poss Pyrrolidinovalerophenone]</th>
      <td>2.135463</td>
    </tr>
    <tr>
      <th>charge_cat[T.Poss3,4 Methylenedioxymethcath]</th>
      <td>-0.186717</td>
    </tr>
    <tr>
      <th>charge_cat[T.Possession of Cannabis]</th>
      <td>-0.394921</td>
    </tr>
    <tr>
      <th>charge_cat[T.Possession of Cocaine]</th>
      <td>0.595596</td>
    </tr>
    <tr>
      <th>charge_cat[T.Viol Injunct Domestic Violence]</th>
      <td>0.135626</td>
    </tr>
    <tr>
      <th>charge_cat[T.Viol Pretrial Release Dom Viol]</th>
      <td>0.231009</td>
    </tr>
    <tr>
      <th>charge_cat[T.arrest case no charge]</th>
      <td>-0.181097</td>
    </tr>
    <tr>
      <th>charge_cat[T.other]</th>
      <td>-0.196721</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.1]</th>
      <td>0.034429</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.3]</th>
      <td>0.087801</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.6]</th>
      <td>0.036031</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.10]</th>
      <td>0.200269</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.14]</th>
      <td>0.709720</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.16]</th>
      <td>0.096648</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.17]</th>
      <td>1.115125</td>
    </tr>
    <tr>
      <th>sex[T.Male]:C(priors_count)[T.26]</th>
      <td>0.221085</td>
    </tr>
    <tr>
      <th>sex[T.Male]:c_charge_degree[T.M]</th>
      <td>0.163792</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Aggravated Battery / Pregnant]</th>
      <td>0.024304</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Burglary Conveyance Unoccup]</th>
      <td>0.516828</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Corrupt Public Servant]</th>
      <td>0.717907</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Crimin Mischief Damage $1000+]</th>
      <td>0.044244</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Driving License Suspended]</th>
      <td>-0.000915</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Driving Under The Influence]</th>
      <td>-0.035058</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Driving While License Revoked]</th>
      <td>-0.094701</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.False Ownership Info/Pawn Item]</th>
      <td>0.480692</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Felony Battery w/Prior Convict]</th>
      <td>-0.096462</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Felony Driving While Lic Suspd]</th>
      <td>0.114971</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Grand Theft in the 3rd Degree]</th>
      <td>0.277304</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Petit Theft]</th>
      <td>0.199624</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Possession of Cannabis]</th>
      <td>-0.051106</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Resist/Obstruct W/O Violence]</th>
      <td>0.272855</td>
    </tr>
    <tr>
      <th>sex[T.Male]:charge_cat[T.Viol Injunct Domestic Violence]</th>
      <td>0.325823</td>
    </tr>
    <tr>
      <th>age</th>
      <td>-0.024242</td>
    </tr>
    <tr>
      <th>sex[T.Male]:age</th>
      <td>-0.014781</td>
    </tr>
    <tr>
      <th>juv_fel_count</th>
      <td>0.302094</td>
    </tr>
    <tr>
      <th>sex[T.Male]:juv_misd_count</th>
      <td>0.181025</td>
    </tr>
    <tr>
      <th>juv_other_count</th>
      <td>0.149152</td>
    </tr>
    <tr>
      <th>sex[T.Male]:juv_other_count</th>
      <td>-0.019176</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now, let’s look at calibration and balance using similar tables and
figures as we did above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">,</span> <span class="n">given_outcome</span><span class="p">,</span> <span class="n">given_pred</span> <span class="o">=</span><span class="n">cm_tables</span><span class="p">(</span>
    <span class="n">lasso_mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">df_test</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_pred</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_outcome</span><span class="p">)</span>

<span class="n">calibration_plot</span><span class="p">(</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)</span>
<span class="n">balance_threshold_plot</span><span class="p">(</span><span class="n">lasso_mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span> <span class="n">df_test</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Portion_of_NoRecid_and_LowRisk</th>
      <td>0.411903</td>
      <td>0.332272</td>
      <td>0.490000</td>
      <td>0.593548</td>
      <td>0.596542</td>
      <td>0.364444</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_LowRisk</th>
      <td>0.141426</td>
      <td>0.161359</td>
      <td>0.121667</td>
      <td>0.096774</td>
      <td>0.048991</td>
      <td>0.165185</td>
    </tr>
    <tr>
      <th>Portion_of_NoRecid_and_HighRisk</th>
      <td>0.190925</td>
      <td>0.182590</td>
      <td>0.201667</td>
      <td>0.200000</td>
      <td>0.250720</td>
      <td>0.175556</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_HighRisk</th>
      <td>0.255745</td>
      <td>0.323779</td>
      <td>0.186667</td>
      <td>0.109677</td>
      <td>0.103746</td>
      <td>0.294815</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(NoRecid|LowRisk)</th>
      <td>0.683284</td>
      <td>0.645361</td>
      <td>0.708434</td>
      <td>0.747967</td>
      <td>0.704082</td>
      <td>0.674897</td>
    </tr>
    <tr>
      <th>P(NoRecid|HighRisk)</th>
      <td>0.356083</td>
      <td>0.332604</td>
      <td>0.394595</td>
      <td>0.468750</td>
      <td>0.320755</td>
      <td>0.359098</td>
    </tr>
    <tr>
      <th>P(Recid|LowRisk)</th>
      <td>0.316716</td>
      <td>0.354639</td>
      <td>0.291566</td>
      <td>0.252033</td>
      <td>0.295918</td>
      <td>0.325103</td>
    </tr>
    <tr>
      <th>P(Recid|HighRisk)</th>
      <td>0.643917</td>
      <td>0.667396</td>
      <td>0.605405</td>
      <td>0.531250</td>
      <td>0.679245</td>
      <td>0.640902</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(LowRisk|NoRecid)</th>
      <td>0.744409</td>
      <td>0.673118</td>
      <td>0.801090</td>
      <td>0.859813</td>
      <td>0.924107</td>
      <td>0.688112</td>
    </tr>
    <tr>
      <th>P(HighRisk|NoRecid)</th>
      <td>0.255591</td>
      <td>0.326882</td>
      <td>0.198910</td>
      <td>0.140187</td>
      <td>0.075893</td>
      <td>0.311888</td>
    </tr>
    <tr>
      <th>P(LowRisk|Recid)</th>
      <td>0.427441</td>
      <td>0.360587</td>
      <td>0.519313</td>
      <td>0.645833</td>
      <td>0.707317</td>
      <td>0.373228</td>
    </tr>
    <tr>
      <th>P(HighRisk|Recid)</th>
      <td>0.572559</td>
      <td>0.639413</td>
      <td>0.480687</td>
      <td>0.354167</td>
      <td>0.292683</td>
      <td>0.626772</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/8ef65116940ddb0c76abb07f389fab661181d6c5200b4438922dc1b065f09230.png" src="../_images/8ef65116940ddb0c76abb07f389fab661181d6c5200b4438922dc1b065f09230.png" />
<img alt="../_images/760bb53a864c691ce8c56bb1d07fe01b740331e0b62e45b75e08aeb7d9c39afd.png" src="../_images/760bb53a864c691ce8c56bb1d07fe01b740331e0b62e45b75e08aeb7d9c39afd.png" />
</div>
</div>
<p>As with COMPAS score, our predictions are well-calibrated, but the
false negative and false positive rates are not well balanced across
racial groups.</p>
<div class="admonition-exercise admonition" id="app-rcd-dir3">
<p class="admonition-title">Exercise</p>
<p>See exercise 3 in the <a class="reference internal" href="#app-rcd-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</section>
<section id="regularizing-to-maximize-balance">
<h3>Regularizing to Maximize Balance<a class="headerlink" href="#regularizing-to-maximize-balance" title="Permalink to this heading">#</a></h3>
<p>Trying to improve balance by ad-hoc modifications will be
difficult.</p>
<p>Let’s try to do it more systematically.</p>
<p>We usually select models and choose regularization to minimize prediction errors.</p>
<p>We can just as well select models and regularization parameters to optimize
some other criteria.</p>
<p>Let’s choose the regularization parameter for lasso to maximize balance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a custom CV criteria to maximize</span>
<span class="k">def</span> <span class="nf">balance_scorer</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="n">df_cv</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">y_true</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">,:]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="s2">&quot;African-American&quot;</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">df_cv</span><span class="o">.</span><span class="n">race</span><span class="o">==</span><span class="s2">&quot;Caucasian&quot;</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">prob</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">fprb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[(</span><span class="n">y_true</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">b</span><span class="p">])</span>
    <span class="n">fprw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[(</span><span class="n">y_true</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">w</span><span class="p">])</span>
    <span class="n">fnrb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[(</span><span class="n">y_true</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">b</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">fnrw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[(</span><span class="n">y_true</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">w</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">fprb</span><span class="o">-</span><span class="n">fprw</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span>
           <span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">fnrb</span><span class="o">-</span><span class="n">fnrw</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span>
           <span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">score_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">df_train</span><span class="p">,</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]}</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">balance_scorer</span><span class="p">,</span> <span class="o">**</span><span class="n">score_params</span><span class="p">,</span> <span class="n">needs_proba</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">grid_cv</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span>
                                              <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                              <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">),</span>
    <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))},</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,)</span>

<span class="n">balance_mod</span><span class="o">=</span><span class="n">grid_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 10 candidates, totalling 50 fits
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plots illustrating regularization parameter choice</span>
<span class="k">def</span> <span class="nf">grid_cv_plot</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">scores</span><span class="o">=</span><span class="n">mod</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">]</span>
    <span class="n">Cdict</span><span class="o">=</span><span class="n">mod</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
    <span class="n">logpenalties</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">Cdict</span><span class="p">])</span>
    <span class="n">colors</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logpenalties</span><span class="p">,</span><span class="n">scores</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;-log(penalty)&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">grid_cv_plot</span><span class="p">(</span><span class="n">balance_mod</span><span class="p">,</span><span class="s2">&quot;CV balance score&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cc8881bfc0c25306342ab1667077197888bd21c7bd105708f64129f3168d3fed.png" src="../_images/cc8881bfc0c25306342ab1667077197888bd21c7bd105708f64129f3168d3fed.png" />
</div>
</div>
<p>We can be perfectly balanced by making the regularization parameter
very large.</p>
<p>Unfortunately, this makes all the predictions identical, so these predictions
are not so useful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">,</span> <span class="n">given_outcome</span><span class="p">,</span> <span class="n">given_pred</span> <span class="o">=</span><span class="n">cm_tables</span><span class="p">(</span>
    <span class="n">balance_mod</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">df_test</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_pred</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:44: RuntimeWarning: invalid value encountered in divide
  pcm = pcm/pcm.sum(axis=axis, keepdims=True)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Portion_of_NoRecid_and_LowRisk</th>
      <td>0.553329</td>
      <td>0.493631</td>
      <td>0.611667</td>
      <td>0.690323</td>
      <td>0.645533</td>
      <td>0.52963</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_LowRisk</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>Portion_of_NoRecid_and_HighRisk</th>
      <td>0.446671</td>
      <td>0.506369</td>
      <td>0.388333</td>
      <td>0.309677</td>
      <td>0.354467</td>
      <td>0.47037</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_HighRisk</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(NoRecid|LowRisk)</th>
      <td>0.553329</td>
      <td>0.493631</td>
      <td>0.611667</td>
      <td>0.690323</td>
      <td>0.645533</td>
      <td>0.52963</td>
    </tr>
    <tr>
      <th>P(NoRecid|HighRisk)</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>P(Recid|LowRisk)</th>
      <td>0.446671</td>
      <td>0.506369</td>
      <td>0.388333</td>
      <td>0.309677</td>
      <td>0.354467</td>
      <td>0.47037</td>
    </tr>
    <tr>
      <th>P(Recid|HighRisk)</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(LowRisk|NoRecid)</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>P(HighRisk|NoRecid)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>P(LowRisk|Recid)</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>P(HighRisk|Recid)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What if we change our CV scoring function to care about both
prediction and balance?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">df_train</span><span class="p">,</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]}</span>
<span class="n">grid_cv</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">scoring</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">balance_scorer</span><span class="p">,</span> <span class="o">**</span><span class="n">score_params</span><span class="p">,</span> <span class="n">needs_proba</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">bf_mod</span><span class="o">=</span><span class="n">grid_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">grid_cv_plot</span><span class="p">(</span><span class="n">bf_mod</span><span class="p">,</span><span class="s2">&quot;CV balance &amp; fit&quot;</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">given_outcome</span><span class="p">,</span> <span class="n">given_pred</span> <span class="o">=</span><span class="n">cm_tables</span><span class="p">(</span>
    <span class="n">bf_mod</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">df_test</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_pred</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">given_outcome</span><span class="p">)</span>
<span class="n">calibration_plot</span><span class="p">(</span><span class="n">bf_mod</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)</span>
<span class="n">balance_threshold_plot</span><span class="p">(</span><span class="n">bf_mod</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span> <span class="n">df_test</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 10 candidates, totalling 50 fits
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
/tmp/ipykernel_2722/1996610596.py:38: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`
  output.loc[:, group] = vals.reshape(-1)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Portion_of_NoRecid_and_LowRisk</th>
      <td>0.405421</td>
      <td>0.328025</td>
      <td>0.483333</td>
      <td>0.574194</td>
      <td>0.561960</td>
      <td>0.365185</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_LowRisk</th>
      <td>0.147908</td>
      <td>0.165605</td>
      <td>0.128333</td>
      <td>0.116129</td>
      <td>0.083573</td>
      <td>0.164444</td>
    </tr>
    <tr>
      <th>Portion_of_NoRecid_and_HighRisk</th>
      <td>0.189747</td>
      <td>0.181529</td>
      <td>0.200000</td>
      <td>0.200000</td>
      <td>0.239193</td>
      <td>0.177037</td>
    </tr>
    <tr>
      <th>Portion_of_Recid_and_HighRisk</th>
      <td>0.256924</td>
      <td>0.324841</td>
      <td>0.188333</td>
      <td>0.109677</td>
      <td>0.115274</td>
      <td>0.293333</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(NoRecid|LowRisk)</th>
      <td>0.681188</td>
      <td>0.643750</td>
      <td>0.707317</td>
      <td>0.741667</td>
      <td>0.701439</td>
      <td>0.673497</td>
    </tr>
    <tr>
      <th>P(NoRecid|HighRisk)</th>
      <td>0.365357</td>
      <td>0.337662</td>
      <td>0.405263</td>
      <td>0.514286</td>
      <td>0.420290</td>
      <td>0.359223</td>
    </tr>
    <tr>
      <th>P(Recid|LowRisk)</th>
      <td>0.318812</td>
      <td>0.356250</td>
      <td>0.292683</td>
      <td>0.258333</td>
      <td>0.298561</td>
      <td>0.326503</td>
    </tr>
    <tr>
      <th>P(Recid|HighRisk)</th>
      <td>0.634643</td>
      <td>0.662338</td>
      <td>0.594737</td>
      <td>0.485714</td>
      <td>0.579710</td>
      <td>0.640777</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>overall</th>
      <th>African-American</th>
      <th>Caucasian</th>
      <th>Hispanic</th>
      <th>Female</th>
      <th>Male</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>P(LowRisk|NoRecid)</th>
      <td>0.732694</td>
      <td>0.664516</td>
      <td>0.790191</td>
      <td>0.831776</td>
      <td>0.870536</td>
      <td>0.689510</td>
    </tr>
    <tr>
      <th>P(HighRisk|NoRecid)</th>
      <td>0.267306</td>
      <td>0.335484</td>
      <td>0.209809</td>
      <td>0.168224</td>
      <td>0.129464</td>
      <td>0.310490</td>
    </tr>
    <tr>
      <th>P(LowRisk|Recid)</th>
      <td>0.424802</td>
      <td>0.358491</td>
      <td>0.515021</td>
      <td>0.645833</td>
      <td>0.674797</td>
      <td>0.376378</td>
    </tr>
    <tr>
      <th>P(HighRisk|Recid)</th>
      <td>0.575198</td>
      <td>0.641509</td>
      <td>0.484979</td>
      <td>0.354167</td>
      <td>0.325203</td>
      <td>0.623622</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../_images/d0a513f1873bd0ef9f1ffc6d375e4b9b41e467e354103a9858e10eced1beec17.png" src="../_images/d0a513f1873bd0ef9f1ffc6d375e4b9b41e467e354103a9858e10eced1beec17.png" />
<img alt="../_images/f65d752e37b42ad0bb28f51688d9dcd0a86a1b036b7587adfb97353163169d88.png" src="../_images/f65d752e37b42ad0bb28f51688d9dcd0a86a1b036b7587adfb97353163169d88.png" />
<img alt="../_images/89fc1f23a50fd149defbea5cc5da26bfb98ad842d11563e0143f9033d38e5ae9.png" src="../_images/89fc1f23a50fd149defbea5cc5da26bfb98ad842d11563e0143f9033d38e5ae9.png" />
</div>
</div>
<div class="admonition-exercise admonition" id="app-rcd-dir4">
<p class="admonition-title">Exercise</p>
<p>See exercise 4 in the <a class="reference internal" href="#app-rcd-ex"><span class="std std-ref">exercise list</span></a>.</p>
</div>
</section>
<section id="tradeoffs-are-inevitable">
<h3>Tradeoffs are Inevitable<a class="headerlink" href="#tradeoffs-are-inevitable" title="Permalink to this heading">#</a></h3>
<p>We could try to tweak our predictions further to improve
balance.</p>
<p>However, motivated in part by this COMPAS example, <span id="id4">[<a class="reference internal" href="#id6" title="Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human Decisions and Machine Predictions*. The Quarterly Journal of Economics, 133(1):237-293, 08 2017. URL: https://dx.doi.org/10.1093/qje/qjx032, arXiv:http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf, doi:10.1093/qje/qjx032.">recidKLL+17</a>]</span> proved
that it is impossible for any prediction algorithm to be both perfectly
balanced and well-calibrated.</p>
<p>Improvements in balance necessarily make calibration worse.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id5">
<dl class="citation">
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id1">recidDMB16</a></span></dt>
<dd><p>William Dieterich, Christina Mendoza, and Tim Brennan. Compas risk scales: demonstrating accuracy equity and predictive parity. <em>Northpoint Inc</em>, 2016.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">recidKLL+17</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>)</span></dt>
<dd><p>Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human Decisions and Machine Predictions*. <em>The Quarterly Journal of Economics</em>, 133(1):237–293, 08 2017. URL: <a class="reference external" href="https://dx.doi.org/10.1093/qje/qjx032">https://dx.doi.org/10.1093/qje/qjx032</a>, <a class="reference external" href="https://arxiv.org/abs/http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf">arXiv:http://oup.prod.sis.lan/qje/article-pdf/133/1/237/24246094/qjx032.pdf</a>, <a class="reference external" href="https://doi.org/10.1093/qje/qjx032">doi:10.1093/qje/qjx032</a>.</p>
</dd>
</dl>
</div>
</section>
<section id="exercises">
<span id="app-rcd-ex"></span><h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<section id="exercise-1">
<h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this heading">#</a></h3>
<p>Can you develop a model that performs better at mimicking their risk scores?</p>
<p>(<a class="reference internal" href="#app-rcd-dir1"><span class="std std-ref">back to text</span></a>)</p>
</section>
<section id="exercise-2">
<h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this heading">#</a></h3>
<p>We made our calibration plot using a held-out test sample. What
do you think would happen if made the calibration plot using the
training sample? Check and see.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create calibration plot using training data</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-rcd-dir2"><span class="std std-ref">back to text</span></a>)</p>
</section>
<section id="exercise-3">
<h3>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this heading">#</a></h3>
<p>Try to improve balance and/or calibration by creating an
alternative prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit your prediction model and plot calibration and balance</span>
</pre></div>
</div>
</div>
</div>
<p>(<a class="reference internal" href="#app-rcd-dir3"><span class="std std-ref">back to text</span></a>)</p>
</section>
<section id="exercise-4">
<h3>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this heading">#</a></h3>
<p>Modify the cross-validation scoring function to see how it affects
calibration and balance.</p>
<p>(<a class="reference internal" href="#app-rcd-dir4"><span class="std std-ref">back to text</span></a>)</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./applications"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive persistent" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/index.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/overview.html">
   Course Description
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/getting_started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/cloud_setup.html">
   Cloud Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/local_install.html">
   Local Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/troubleshooting.html">
   Troubleshooting
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Python Fundamentals
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../python_fundamentals/index.html">
   Python Fundamentals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_fundamentals/basics.html">
   Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_fundamentals/collections.html">
   Collections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_fundamentals/control_flow.html">
   Control Flow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_fundamentals/functions.html">
   Functions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Scientific Computing
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/index.html">
   Scientific Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/numpy_arrays.html">
   Introduction to Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/plotting.html">
   Plotting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/applied_linalg.html">
   Applied Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/randomness.html">
   Randomness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../scientific/optimization.html">
   Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pandas
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/index.html">
   DataFrames and Series in Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/basics.html">
   Basic Functionality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/the_index.html">
   The Index
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/storage_formats.html">
   Storage Formats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/data_clean.html">
   Cleaning Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/reshape.html">
   Reshape
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/merge.html">
   Merge
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/groupby.html">
   GroupBy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pandas/timeseries.html">
   Time series
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science Tools
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/index.html">
   Data Science Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/matplotlib.html">
   Intermediate Plotting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/maps.html">
   Mapping in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/visualization_rules.html">
   Data Visualization: Rules and Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools/classification.html">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_in_economics.html">
   Machine Learning in Economics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="networks.html">
   Social and Economic Networks
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   Case Study: Recidivism
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="working_with_text.html">
   Working with Text
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heterogeneity.html">
   Heterogeneous Effects
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="../index.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="../search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/applications/recidivism.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li data-tippy-content="Download PDF" onClick="window.print()"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-datascience.myst/tree/main/lectures/applications/recidivism.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://mybinder.org/v2/gh/QuantEcon/lecture-datascience.notebooks/main?urlpath=tree/applications/recidivism.ipynb">BinderHub</option>
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-datascience.notebooks/blob/main/applications/recidivism.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-datascience.notebooks" data-urlpath="tree/lecture-datascience.notebooks/applications/recidivism.ipynb" data-branch=main>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-datascience.notebooks/main?urlpath=tree/applications/recidivism.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "applications/recidivism";
                const repoURL = "https://github.com/QuantEcon/lecture-datascience.notebooks";
                const urlPath = "tree/lecture-datascience.notebooks/applications/recidivism.ipynb";
                const branch = "main"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>