{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfffc65",
   "metadata": {},
   "source": [
    "# Working with Text\n",
    "\n",
    "**Author**\n",
    "\n",
    "> - [Paul Schrimpf *UBC*](https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/)  \n",
    "\n",
    "\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- [Visualization Rules](https://datascience.quantecon.org/visualization_rules.html)  \n",
    "- [Regression](https://datascience.quantecon.org/regression.html)  \n",
    "- [Classification](https://datascience.quantecon.org/classification.html)  \n",
    "- [Maps](https://datascience.quantecon.org/maps.html)  \n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Use text as features for classification  \n",
    "- Understand latent topic analysis  \n",
    "- Use folium to create an interactive map  \n",
    "- Request and combine json data from a web server  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d74a9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Uncomment following line to install on colab\n",
    "#! pip install fiona geopandas xgboost gensim folium pyLDAvis descartes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a7dc9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many data sources contain both numerical data and text.\n",
    "\n",
    "We can use text to create features for any of the prediction methods\n",
    "that we have discussed.\n",
    "\n",
    "Doing so requires encoding text into some numerical representation.\n",
    "\n",
    "A good encoding preserves the meaning of the original text, while\n",
    "keeping dimensionality manageable.\n",
    "\n",
    "In this lecture, we will learn how to work with text through an\n",
    "application — predicting fatalities from avalanche\n",
    "forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1ee48",
   "metadata": {},
   "source": [
    "## Avalanches\n",
    "\n",
    "Snow avalanches are a hazard in the mountains. Avalanches can be\n",
    "partially predicted based on snow conditions, weather, and\n",
    "terrain. [Avalanche Canada](https://www.avalanche.ca/map) produces\n",
    "daily avalanche forecasts for various Canadian mountainous regions.\n",
    "These forecasts consist of 1-5 ratings for each of three\n",
    "elevation bands, as well as textual descriptions of recent avalanche\n",
    "observations, snowpack, and weather. Avalanche Canada also\n",
    "maintains a list of [fatal avalanche incidents](https://www.avalanche.ca/incidents/) . In this lecture, we will\n",
    "attempt to predict fatal incidents from the text of avalanche\n",
    "forecasts. Since fatal incidents are rare, this prediction task will\n",
    "be quite difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fa413",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedaff7d",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Avalanche Canada has an unstable json api. The api seems to be largely\n",
    "tailored to displaying the information on various Avalanche Canada\n",
    "websites, which does not make it easy to obtain large amounts of\n",
    "data. Nonetheless, getting information from the API is easier than\n",
    "scraping the website. Generally, whenever you’re considering scraping\n",
    "a website, you should first check whether the site has an API available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e35662f",
   "metadata": {},
   "source": [
    "#### Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b00c1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Get data on avalanche forecasts and incidents from Avalanche Canada\n",
    "# Avalanche Canada has an unstable public api\n",
    "# https://github.com/avalanche-canada/ac-web\n",
    "# Since API might change, this code might break\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Incidents\n",
    "url = \"http://incidents.avalanche.ca/public/incidents/?format=json\"\n",
    "req = urllib.request.Request(url)\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    result = json.loads(response.read().decode('utf-8'))\n",
    "incident_list = result[\"results\"]\n",
    "while (result[\"next\"] != None):\n",
    "    req = urllib.request.Request(result[\"next\"])\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    incident_list = incident_list + result[\"results\"]\n",
    "incidents_brief = pd.DataFrame.from_dict(incident_list,orient=\"columns\")\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8\n",
    "incidents_brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d9cce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# We can get more information about these incidents e.g. \"https://www.avalanche.ca/incidents/37d909e4-c6de-43f1-8416-57a34cd48255\"\n",
    "# this information is also available through the API\n",
    "def get_incident_details(id):\n",
    "    url = \"http://incidents.avalanche.ca/public/incidents/{}?format=json\".format(id)\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    return(result)\n",
    "\n",
    "\n",
    "incidentsfile = \"https://datascience.quantecon.org/assets/data/avalanche_incidents.csv\"\n",
    "\n",
    "# To avoid loading the avalanche Canada servers, we save the incident details locally.\n",
    "if (not os.path.isfile(incidentsfile)):\n",
    "    incident_detail_list = incidents_brief.id.apply(get_incident_details).to_list()\n",
    "    incidents = pd.DataFrame.from_dict(incident_detail_list, orient=\"columns\")\n",
    "    incidents.to_csv(incidentsfile)\n",
    "else:\n",
    "    incidents = pd.read_csv(incidentsfile)\n",
    "\n",
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40942336",
   "metadata": {},
   "source": [
    "Many incidents include coordinates, but others do not. Most\n",
    "however, do include a place name. We can use [Natural Resources Canada’s\n",
    "Geolocation Service](https://www.nrcan.gc.ca/earth-sciences/geography/topographic-information/geolocalisation-service/17304)\n",
    "to retrieve coordinates from place names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953abb8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# geocode locations without coordinates\n",
    "def geolocate(location, province):\n",
    "    url = \"http://geogratis.gc.ca/services/geolocation/en/locate?q={},%20{}\"\n",
    "    req = urllib.request.Request(url.format(urllib.parse.quote(location),province))\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = json.loads(response.read().decode('utf-8'))\n",
    "    if (len(result)==0):\n",
    "        return([None,None])\n",
    "    else:\n",
    "        return(result[0]['geometry']['coordinates'])\n",
    "if not \"alt_coord\" in incidents.columns:\n",
    "    incidents[\"alt_coord\"] = [\n",
    "        geolocate(incidents.location[i], incidents.location_province[i])\n",
    "        for i in incidents.index\n",
    "    ]\n",
    "    incidents.to_csv(incidentsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d32bc9",
   "metadata": {},
   "source": [
    "Now that we have incident data, let’s create some figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b75210",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# clean up activity names\n",
    "incidents.group_activity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f137c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents.group_activity=incidents.group_activity.replace(\"Ski touring\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Out-of-Bounds Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Lift Skiing Closed\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Skiing\",\"Backcountry Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Snowshoeing and Hiking\",\"Snowshoeing & Hiking\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Mechanized Skiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Heliskiing\",\"Heli or Cat Skiing\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"At Outdoor Worksite\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Control Work\",\"Work\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Hunting/Fishing\",\"Other Recreational\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Car/Truck on Road\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Inside Building\",\"Car/Truck/Building\")\n",
    "incidents.group_activity=incidents.group_activity.replace(\"Outside Building\",\"Car/Truck/Building\")\n",
    "\n",
    "\n",
    "incidents.group_activity.unique()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "incidents.groupby(['group_activity']).id.count().plot(kind='bar', title=\"Incidents by Activity\", ax=ax[0])\n",
    "incidents.groupby(['group_activity']).num_fatal.sum().plot(kind='bar', title=\"Deaths by Activity\", ax=ax[1], color=colors[1])\n",
    "ax[0].set_xlabel(None)\n",
    "ax[1].set_xlabel(None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e8585",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents[\"date\"] = pd.to_datetime(incidents.ob_date)\n",
    "incidents[\"year\"] = incidents.date.apply(lambda x: x.year)\n",
    "incidents.date = incidents.date.dt.date\n",
    "colors=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "f = incidents.groupby([\"year\"]).num_fatal.sum()\n",
    "n = incidents.groupby([\"year\"]).id.count()\n",
    "yearstart=1950\n",
    "f=f[f.index>yearstart]\n",
    "n=n[n.index>yearstart]\n",
    "fig,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "n.plot(ax=ax)\n",
    "f.plot(ax=ax)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.annotate(\"Incidents\", (2010, 4), color=colors[0])\n",
    "ax.annotate(\"Deaths\", (2011, 15), color=colors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed7770",
   "metadata": {},
   "source": [
    "#### Mapping Incidents\n",
    "\n",
    "Since the incident data includes coordinates, we might as well make a\n",
    "map too. Unfortunately, some latitude and longitudes contain obvious errors.\n",
    "Here, we try to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb69f5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# fix errors in latitude, longitude\n",
    "latlon = incidents.location_coords\n",
    "def makenumeric(cstr):\n",
    "    if cstr is None:\n",
    "        return([None,None])\n",
    "    elif (type(cstr)==str):\n",
    "        return([float(s) for s in re.findall(r'-?\\d+\\.?\\d*',cstr)])\n",
    "    else:\n",
    "        return(cstr)\n",
    "\n",
    "latlon = latlon.apply(makenumeric)\n",
    "\n",
    "def good_lat(lat):\n",
    "    return(lat >= 41.6 and lat <= 83.12) # min & max for Canada\n",
    "\n",
    "def good_lon(lon):\n",
    "    return(lon >= -141 and lon<= -52.6)\n",
    "\n",
    "def fixlatlon(c):\n",
    "    if (len(c)<2 or type(c[0])!=float or type(c[1])!=float):\n",
    "        c = [None, None]\n",
    "        return(c)\n",
    "    lat = c[0]\n",
    "    lon = c[1]\n",
    "    if not good_lat(lat) and good_lat(lon):\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lon):\n",
    "        lon = -lon\n",
    "    if not good_lon(lon) and good_lon(lat):\n",
    "        tmp = lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lon(lon) and good_lon(-lat):\n",
    "        tmp = -lat\n",
    "        lat = lon\n",
    "        lon = tmp\n",
    "    if not good_lat(lat) or not good_lon(lon):\n",
    "        c[0] = None\n",
    "        c[1] = None\n",
    "    else:\n",
    "        c[0] = lat\n",
    "        c[1] = lon\n",
    "    return(c)\n",
    "\n",
    "incidents[\"latlon\"] = latlon.apply(fixlatlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acc3c0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def foo(c, a):\n",
    "    if (type(a)==str):\n",
    "        a = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*',a)]\n",
    "    if len(a) <2:\n",
    "        a = [None,None]\n",
    "    return([a[1],a[0]] if type(c[0])!=float else c)\n",
    "incidents[\"latlon_filled\"]=[foo(c,a) for c,a in zip(incidents[\"latlon\"],incidents[\"alt_coord\"])]\n",
    "nmiss = sum([a[0]==None for a in incidents.latlon_filled])\n",
    "n = len(incidents.latlon_filled)\n",
    "print(\"{} of {} incidents have latitude & longitude\".format(n-nmiss, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d19199",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# download forecast region definitions\n",
    "# req = urllib.request.Request(\"https://www.avalanche.ca/api/forecasts\")\n",
    "# The above link doesn't work since COVID-19 lockdown. Currently we use an old cached version instead\n",
    "#req = (\"https://web.archive.org/web/20150319031605if_/http://www.avalanche.ca/api/forecasts\")\n",
    "#with urllib.request.urlopen(req) as response:\n",
    "#    forecastregions = json.loads(response.read().decode('utf-8'))\n",
    "req = \"https://faculty.arts.ubc.ca/pschrimpf/forecast-regions2015.json\"\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions2015 = json.loads(response.read().decode('utf-8'))\n",
    "\n",
    "req = \"https://faculty.arts.ubc.ca/pschrimpf/forecast-regions2019.json\"\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions2019 = json.loads(response.read().decode('utf-8'))\n",
    "\n",
    "forecastregions = regions2019\n",
    "ids = [r['id'] for r in forecastregions['features']]\n",
    "for r in regions2015['features'] :\n",
    "     if not r['id'] in ids :\n",
    "            forecastregions['features'].append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822102f",
   "metadata": {},
   "source": [
    "You may have to uncomment the second line below if  folium is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2eb1f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Map forecast regions and incidents\n",
    "#!pip install --user folium\n",
    "import folium\n",
    "import matplotlib\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "fmap = folium.Map(location=[60, -98],\n",
    "                            zoom_start=3,\n",
    "                            tiles='Stamen Terrain')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    regions_tmp = json.loads(response.read().decode('utf-8'))\n",
    "folium.GeoJson(regions_tmp,\n",
    "               tooltip=folium.GeoJsonTooltip(fields=[\"name\"], aliases=[\"\"]),\n",
    "               highlight_function=lambda x: { 'weight': 10},\n",
    "              style_function=lambda x: {'weight':1}).add_to(fmap)\n",
    "activities = incidents.group_activity.unique()\n",
    "for i in incidents.index:\n",
    "    if incidents.latlon_filled[i][0] is not None and  incidents.latlon_filled[i][1] is not None:\n",
    "        cindex=[j for j,x in enumerate(activities) if x==incidents.group_activity[i]][0]\n",
    "        txt = \"{}, {}<br>{} deaths\"\n",
    "        txt = txt.format(incidents.group_activity[i],\n",
    "                        incidents.ob_date[i],\n",
    "                        incidents.num_fatal[i]\n",
    "                        )\n",
    "        pop = folium.Popup(incidents.comment[i], parse_html=True, max_width=400)\n",
    "        folium.CircleMarker(incidents.latlon_filled[i],\n",
    "                      tooltip=txt,\n",
    "                      popup=pop,\n",
    "                      color=matplotlib.colors.to_hex(cmap(cindex)), fill=True, radius=5).add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fec3f1",
   "metadata": {},
   "source": [
    "Take a moment to click around the map and read about some of the incidents.\n",
    "\n",
    "Between presenting this information on a map and the list on [https://www.avalanche.ca/incidents/](https://www.avalanche.ca/incidents/) ,\n",
    "which do you prefer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be84330",
   "metadata": {},
   "source": [
    "#### Matching Incidents to Regions\n",
    "\n",
    "Later, we will want to match incidents to forecasts, so let’s find the closest region to each incident.\n",
    "\n",
    "Note that distance here will be in units of latitude, longitude (or\n",
    "whatever coordinate system we use). At the equator, a distance of 1 is\n",
    "approximately 60 nautical miles.\n",
    "\n",
    "Since longitude lines get closer together farther from the equator,\n",
    "these distances will be understated the further North you go.\n",
    "\n",
    "This is not much of a problem if we’re just finding the\n",
    "nearest region, but if we care about accurate distances, we should\n",
    "re-project the latitude and longitude into a different coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f05c8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Match incidents to nearest forecast regions.\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "point = Point(incidents.latlon_filled[0][1],incidents.latlon_filled[0][0])\n",
    "def distances(latlon):\n",
    "    point=Point(latlon[1],latlon[0])\n",
    "    df = pd.DataFrame.from_dict([{'id':feature['id'],\n",
    "                                  'distance':shape(feature['geometry']).distance(point)} for\n",
    "                                 feature in forecastregions['features']])\n",
    "    return(df)\n",
    "def foo(x):\n",
    "    if (x[0]==None):\n",
    "        return(None)\n",
    "    d = distances(x)\n",
    "    return(d.id[d.distance.idxmin()])\n",
    "incidents['nearest_region'] = incidents.latlon_filled.apply(foo)\n",
    "incidents['nearest_distance'] = incidents.latlon_filled.apply(lambda x: None if x[0]==None else distances(x).distance.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260070d5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64251e01",
   "metadata": {},
   "source": [
    "#### Forecast Data\n",
    "\n",
    "We’ll now download all forecasts for all regions since November 2011 (roughly the earliest data available).\n",
    "\n",
    "We can only request one forecast at a time, so this takes many hours to download.\n",
    "\n",
    "To make this process run more quickly for readers, we ran the code ourselves and then stored the data in the cloud.\n",
    "\n",
    "The function below will fetch all the forecasts from the cloud storage location and save them to a folder\n",
    "named `avalanche_forecasts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec2888",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def download_cached_forecasts():\n",
    "    # download the zipped file and unzip it here\n",
    "    url = \"https://datascience.quantecon.org/assets/data/avalanche_forecasts.zip?raw=true\"\n",
    "    with requests.get(url) as res:\n",
    "        if not res.ok:\n",
    "            raise ValueError(\"failed to download the cached forecasts\")\n",
    "        with zipfile.ZipFile(io.BytesIO(res.content)) as z:\n",
    "            for f in z.namelist():\n",
    "                if (os.path.isfile(f) and z.getinfo(f).file_size < os.stat(f).st_size):\n",
    "                    warnings.warn(f\"'File $f exists and is larger than version in cache. Not replacing.\")\n",
    "                else :\n",
    "                    z.extract(f)\n",
    "\n",
    "download_cached_forecasts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228824cf",
   "metadata": {},
   "source": [
    "The code below is what we initially ran to obtain all the forecasts.\n",
    "\n",
    "You will notice that this code checks to see whether the files can be found in the `avalanche_forecasts`\n",
    "directory (they can if you ran the `download_cached_forecasts` above!) and will only download them if they aren’t found.\n",
    "\n",
    "You can experiment with this caching by deleting one or more files from the `avalanche_forecasts`\n",
    "folder and re-running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97485a7f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Functions for downloading forecasts from Avalanche Canada\n",
    "\n",
    "def get_forecast(date, region):\n",
    "    url = \"https://www.avalanche.ca/api/bulletin-archive/{}/{}.json\".format(date.isoformat(),region)\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            result = json.loads(response.read().decode('utf-8'))\n",
    "        return(result)\n",
    "    except:\n",
    "        return(None)\n",
    "\n",
    "def get_forecasts(start, end, region):\n",
    "    day = start\n",
    "    forecasts = []\n",
    "    while(day<=end and day<end.today()):\n",
    "        #print(\"working on {}, {}\".format(region,day))\n",
    "        forecasts = forecasts + [get_forecast(day, region)]\n",
    "        #print(\"sleeping\")\n",
    "        time.sleep(0.1) # to avoid too much load on Avalanche Canada servers\n",
    "        day = day + pd.Timedelta(1,\"D\")\n",
    "    return(forecasts)\n",
    "\n",
    "def get_season(year, region):\n",
    "    start_month = 11\n",
    "    start_day = 20\n",
    "    last_month = 5\n",
    "    last_day = 1\n",
    "    if (not os.path.isdir(\"avalanche_forecasts\")):\n",
    "        os.mkdir(\"avalanche_forecasts\")\n",
    "    seasonfile = \"avalanche_forecasts/{}_{}-{}.json\".format(region, year, year+1)\n",
    "    if (not os.path.isfile(seasonfile)):\n",
    "        startdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year, start_month, start_day))\n",
    "        lastdate = pd.to_datetime(\"{}-{}-{} 12:00\".format(year+1, last_month, last_day))\n",
    "        season = get_forecasts(startdate,lastdate,region)\n",
    "        with open(seasonfile, 'w') as outfile:\n",
    "            json.dump(season, outfile, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(seasonfile, \"rb\") as json_data:\n",
    "            season = json.load(json_data)\n",
    "    return(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6fd6e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "forecastlist=[]\n",
    "\n",
    "for year in range(2011,2019):\n",
    "    print(\"working on {}\".format(year))\n",
    "    for region in [region[\"id\"] for region in forecastregions[\"features\"]]:\n",
    "        forecastlist = forecastlist + get_season(year, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8e3d3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# convert to DataFrame and extract some variables\n",
    "forecasts = pd.DataFrame.from_dict([f for f in forecastlist if not f==None],orient=\"columns\")\n",
    "\n",
    "forecasts[\"danger_date\"] = forecasts.dangerRatings.apply(lambda r: r[0][\"date\"])\n",
    "forecasts[\"danger_date\"] = pd.to_datetime(forecasts.danger_date, utc=True).dt.date\n",
    "forecasts[\"danger_alpine\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"alp\"])\n",
    "forecasts[\"danger_treeline\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"tln\"])\n",
    "forecasts[\"danger_belowtree\"]=forecasts.dangerRatings.apply(lambda r: r[0][\"dangerRating\"][\"btl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde744d1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc450b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# merge incidents to forecasts\n",
    "adf = pd.merge(forecasts, incidents, how=\"left\",\n",
    "               left_on=[\"region\",\"danger_date\"],\n",
    "               right_on=[\"nearest_region\",\"date\"],\n",
    "              indicator=True)\n",
    "adf[\"incident\"] = adf._merge==\"both\"\n",
    "print(\"There were {} incidents matched with forecasts data. These occured on {}% of day-regions with forecasts\".format(adf.incident.sum(),adf.incident.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193f818",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ratings=sorted(adf.danger_alpine.unique())\n",
    "ava_colors = [\"#52BA4A\", \"#FFF300\", \"#F79218\", \"#EF1C29\", \"#1A1A1A\", \"#BFBFBF\"]\n",
    "for x in [\"danger_alpine\", \"danger_treeline\", \"danger_belowtree\"]:\n",
    "    fig=sns.catplot(x=x, kind=\"count\",col=\"incident\", order=ratings, data=adf, sharey=False,\n",
    "                    palette=ava_colors, height=3, aspect=2)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    fig.fig.suptitle(x.replace(\"danger_\",\"\"))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f8f61",
   "metadata": {},
   "source": [
    "## Predicting Incidents from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622d3af",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The first step when using text as data is to pre-process the text.\n",
    "\n",
    "In preprocessing, we will:\n",
    "\n",
    "1. Clean: Remove unwanted punctuation and non-text characters.  \n",
    "1. Tokenize: Break sentences down into words.  \n",
    "1. Remove “stopwords”: Eliminate common words that actually provide no information, like “a” and “the”.  \n",
    "1. Lemmatize words: Reduce words to their dictionary “lemma” e.g. “snowing” and “snowed” both become snow (verb).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e160d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Remove stopwords (the, a, is, etc)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords=stopwords.union(set(string.punctuation))\n",
    "# Lemmatize words e.g. snowed and snowing are both snow (verb)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "def text_prep(txt):\n",
    "    soup = BeautifulSoup(txt, \"lxml\")\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    txt=soup.text # remove html tags\n",
    "    txt = txt.lower()\n",
    "    tokens = [token for token in nltk.tokenize.word_tokenize(txt)]\n",
    "    tokens = [token for token in tokens if not token in stopwords]\n",
    "    #tokens = [token for token in tokens if not token ]\n",
    "    tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "    if (len(tokens)==0):\n",
    "        tokens = [\"EMPTYSTRING\"]\n",
    "    return(tokens)\n",
    "\n",
    "text_prep(forecasts.highlights[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a3c10",
   "metadata": {},
   "source": [
    "Now, let’s apply this to all avalanche summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19acef0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e6ff6",
   "metadata": {},
   "source": [
    "Let’s make a bar plot of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f57aa",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "wf = nltk.FreqDist([word for doc in text_data for word in doc]).most_common(20)\n",
    "words = [x[0] for x in wf]\n",
    "cnt = [x[1] for x in wf]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.bar(range(len(words)), cnt);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Most common words in avalanche summaries');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4c733",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "The “bag of words” approach is the simplest way to convert a collection of processed text\n",
    "documents to a feature matrix. We view\n",
    "each document as a bag of words, and our feature matrix\n",
    "counts how many times each word appears. This method is called a “bag of words”\n",
    "because we ignore the document’s word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca90e7b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "text_data = [text_prep(txt) for txt in adf.avalancheSummary]\n",
    "y = adf.incident\n",
    "X = vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6f6ce",
   "metadata": {},
   "source": [
    "We can also perform more complicated feature engineering. One extension of the “bag of words” method\n",
    "is to consider counts of pairs or triples of consecutive words. These\n",
    "are called n-grams and can be created by setting the `n_gram`\n",
    "argument to `CountVectorizer`. Another alternative might be to accommodate\n",
    "the fact that common words will inherently have higher counts by using\n",
    "term-frequency inverse-document-frequency (see below).\n",
    "\n",
    "After creating our feature matrix, we can now apply any classification\n",
    "method to predict incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29e368",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "A common text data classifier is the Naive Bayes classifier.\n",
    "This classifier predicts incidents using Bayes’ rules.\n",
    "\n",
    "$$\n",
    "P(incident | words) = \\frac{P(words|incident) P(incidents)}{P(words)}\n",
    "$$\n",
    "\n",
    "The classifier is naive, though; it assumes words are independent of one another in any given incident.\n",
    "\n",
    "$$\n",
    "P(words|incident) = \\prod_{w \\in words} P(w|incident)\n",
    "$$\n",
    "\n",
    "Although this assumption is implausible for text, the Naive Bayes\n",
    "classifier can be computed extremely quickly, and sometimes quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e0c89",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82076c0f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(Xtrain,ytrain)\n",
    "np.mean(classifier.predict(Xtest)==ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a892e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(ytest, classifier.predict(Xtest)))\n",
    "print(metrics.classification_report(ytest, classifier.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4a947",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# print text with highest predicted probabilities\n",
    "phat=classifier.predict_proba(X)[:,1]\n",
    "def remove_html(txt):\n",
    "    soup = BeautifulSoup(txt, \"lxml\")\n",
    "    [s.extract() for s in soup('style')] # remove css\n",
    "    return(soup.text)\n",
    "docs = [remove_html(txt) for txt in adf.avalancheSummary]\n",
    "txt_high = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0],reverse=True)]\n",
    "txt_high[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b893b8a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# print text with lowest predicted probabilities\n",
    "txt_low = [(_,x) for _, x in sorted(zip(phat,docs), key=lambda pair: pair[0])]\n",
    "txt_low[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b1fa3",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See exercise 1 in the [exercise list](#app-txt-ex).\n",
    "\n",
    "Predicting deaths from forecast text is very difficult because deaths\n",
    "are so rare. A prediction exercise more likely to succeed would be to\n",
    "predict the avalanche rating from the forecast text. However,\n",
    "doing so is a very\n",
    "artificial task, with little practical use.\n",
    "\n",
    "Another alternative would be to gather more data on non-fatal\n",
    "avalanches. Avalanche Canada also has user-submitted “Mountain\n",
    "Information Network” reports. These reports include observations of\n",
    "natural avalanches and information on non-fatal avalanche\n",
    "incidents. Since the data is user-submitted, it is messy and more\n",
    "difficult to work with. Nonetheless, working with it would be\n",
    "good practice and could lead to some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a97017",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "The regression and classification methods that we have seen so far are\n",
    "examples of supervised learning — we are trying to predict an observed outcome.\n",
    "In unsupervised learning, we do not have an\n",
    "observed outcome to predict. Instead, we try to find informative\n",
    "patterns in the data. Unsupervised learning can be particularly useful\n",
    "with text data. We will look at two related techniques for topic\n",
    "modeling. These techniques attempt to extract distinct topics from a\n",
    "collection of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb130888",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent semantic analysis is used by some search engines to rank\n",
    "the similarities among documents. Latent semantic analysis begins with a\n",
    "term document matrix, $ X $. The term document matrix is a number\n",
    "of documents by number of terms matrix where the i,jth entry is the\n",
    "measure of how often term j appears in document i. This could be the\n",
    "same bag of words feature matrix we constructed above, or it could be\n",
    "some other measure. For this example, we will use the term-frequency,\n",
    "inverse-document-frequency representation.\n",
    "\n",
    "$$\n",
    "x^{tfidf}_{ij} = \\frac{\\text{occurences of term j in document\n",
    "i}}{\\text{length of document i}} \\log \\left(\\frac{\\text{number of\n",
    "documents}}{\\text{number of documents containing term j}}\\right)\n",
    "$$\n",
    "\n",
    "Given a term document matrix, $ X $, latent semantic analysis\n",
    "computes a lower rank approximation to $ X $ through the singular\n",
    "value decomposition. This lower rank approximation can potentially be\n",
    "interpreted or used instead of $ X $ for other learning\n",
    "algorithms. In other contexts, similar decompositions are referred to\n",
    "as principal components analysis or factor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def39df",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LSA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "X = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in text_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc619f20",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1e15d",
   "metadata": {},
   "source": [
    "Here, we have computed a rank 10 approximation to the tf-idf matrix. We\n",
    "can see how much variance of the original matrix that our 10\n",
    "components reproduce. We can also look at how all terms in the\n",
    "document contribute to each of the 10 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185f2bc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "print(svd_model.explained_variance_ratio_)\n",
    "print(svd_model.explained_variance_ratio_.cumsum())\n",
    "terms = tfidf_vectorizer.get_feature_names_out() \n",
    "comp_label=[]\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    message = \"\"\n",
    "    for t in sorted_terms:\n",
    "        message = message + \"{:.2f} * {} + \".format(t[1],t[0])\n",
    "    print(message)\n",
    "    comp_label.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e71d7",
   "metadata": {},
   "source": [
    "Finally, we can attempt to visualize the components.\n",
    "\n",
    "The LSA reduced the dimensionality of the representation of documents\n",
    "from thousands (of term-frequency inverse-document-frequency) to ten\n",
    "components. While ten is more manageable than thousands, it is still too\n",
    "many dimensions to effectively visualize. t-SNE is a technique to further\n",
    "reduce dimensionality. t-SNE is a nonlinear data transformation\n",
    "from many dimensions to 2, while attempting to preserve the\n",
    "original clustering of their original domain. In other words, if a\n",
    "set of documents are closely clustered in the 10 dimensional\n",
    "LSA space, then they will also be close together in the 2 dimensional\n",
    "t-SNE representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b482f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lsa_topic_matrix = svd_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551afed3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "nplot = 2000 # reduce the size of the data to speed computation and make the plot less cluttered\n",
    "lsa_topic_sample = lsa_topic_matrix[np.random.choice(lsa_topic_matrix.shape[0], nplot, replace=False)]\n",
    "tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=500,\n",
    "                      n_iter=1000, verbose=10, random_state=0, angle=0.75)\n",
    "tsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bd4c6",
   "metadata": {},
   "source": [
    "The t-SNE model creates a\n",
    "non-linear projection from our 10 dimensional LSA topics onto two dimensional space.\n",
    "It can be useful for visualizing high-dimensional data. One word of caution:\n",
    "the output of the t-SNE model can depend on the parameters of the\n",
    "algorithm. Failure to see clear clusters in the t-SNE visualization\n",
    "could mean either the original data was not clustered in higher\n",
    "dimensional space or that the t-SNE algorithm parameters were\n",
    "chosen poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b38809",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('Paired')\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
    "n_topics=len(svd_model.components_)\n",
    "lsa_keys = np.argmax(lsa_topic_sample, axis=1)\n",
    "ax[0].scatter(x=tsne_lsa_vectors[:,0],y=tsne_lsa_vectors[:,1], color=[cmap(i) for i in lsa_keys], alpha=0.8)\n",
    "bbox_props = dict(boxstyle=\"round4,pad=0.1\", lw=0.2, fc=\"white\")\n",
    "for i in range(n_topics):\n",
    "    m = tsne_lsa_vectors[lsa_keys==i, :].mean(axis=0)\n",
    "    ax[0].text(m[0], m[1], str(i), ha=\"center\", va=\"center\",\n",
    "               size=15, color=cmap(i),\n",
    "               bbox=bbox_props)\n",
    "    ax[1].text(0,1-(i+1)*1/(n_topics+1),\"Topic \" + str(i) + \" : \"+ comp_label[i],ha=\"left\", va=\"center\", color=cmap(i))\n",
    "    ax[1].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b9afb",
   "metadata": {},
   "source": [
    "From this plot, we can immediately see two things. First, most documents\n",
    "are closest to topic 0. Second, most topics are not\n",
    "well-separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0034c68",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See exercise 2 in the [exercise list](#app-txt-ex)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0023584",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Analysis\n",
    "\n",
    "Latent dirichlet analysis (LDA) produces similar outputs as latent semantic\n",
    "analysis, but LDA often produces nicer results. The statistical theory\n",
    "underlying LSA is built on continuous $ X $ features. LDA uses\n",
    "similar ideas, but takes into account that text is discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c5349",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "# gensim works with a list of lists of tokens\n",
    "text_data = [text_prep(txt) for txt in forecasts.avalancheSummary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fa413",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# convert to bag of words\n",
    "dictionary = gensim.corpora.Dictionary(text_data)\n",
    "bow_data = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276904b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(bow_data, num_topics = 5, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323628d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = pyLDAvis.gensim_models.prepare(ldamodel, bow_data, dictionary)\n",
    "lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c9be1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See exercise 3 in the [exercise list](#app-txt-ex)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367bc76",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "See exercise 4 in the [exercise list](#app-txt-ex).\n",
    "\n",
    "\n",
    "<a id='app-txt-ex'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004ba0c",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6158c",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use another classification method to predict incidents. Check whether\n",
    "your method outperforms the Naive Bayes classifier.\n",
    "\n",
    "([back to text](#app-txt-dir1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd687bc8",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Apply LSA to the weather or snowpack descriptions. Can you notice\n",
    "any patterns?\n",
    "\n",
    "([back to text](#app-txt-dir2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf4640",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Apply LDA to the weather or snowpack descriptions. Can you notice\n",
    "any patterns?\n",
    "\n",
    "([back to text](#app-txt-dir3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95899e3",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Use the reduced rank representation of text from LSA or LDA as a\n",
    "feature matrix to predict avalanche incidents. Compare the\n",
    "performance with the bag of words feature matrix.\n",
    "\n",
    "([back to text](#app-txt-dir4))"
   ]
  }
 ],
 "metadata": {
  "date": 1680209447.0701494,
  "filename": "working_with_text.md",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Working with Text"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}